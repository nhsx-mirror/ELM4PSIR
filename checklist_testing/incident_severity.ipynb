{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99bf56d4",
   "metadata": {},
   "source": [
    "## CheckListing Classification Models\n",
    "\n",
    "### General Idea\n",
    "\n",
    "Models have been trained to have some idea of the severity of the incident reports and that subtle perturbations should not hinder this ability, and potentially negation should be understood - or not. \n",
    "\n",
    "This notebook was used for our own dataset and task, but can be adapted to any sort of classification model, such as a sentiment analysis model. \n",
    "\n",
    "**NOTE** - This notebook shows an example for a binary classification problem and would require some adaptation to work with a multi-class problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00950e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import re\n",
    "\n",
    "import checklist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from checklist.editor import Editor\n",
    "from checklist.expect import Expect\n",
    "from checklist.perturb import Perturb\n",
    "from checklist.test_suite import TestSuite\n",
    "from checklist.test_types import DIR, INV, MFT\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f473a9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data directory to data of interest\n",
    "data_dir = \"./data/\"\n",
    "df = pd.read_csv(f\"{data_dir}/train.csv\", nrows=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3adc4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data read in\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39931446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in model and tokenizer\n",
    "cache_dir = \".cache\"  # cache directory for transformer models\n",
    "model_path_or_name = \"roberta-base\"  # this is where you will want to load in a task specific trained model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_path_or_name, cache_dir=cache_dir\n",
    ")\n",
    "# sentiment analysis is a general name in Huggingface to load the pipeline for text classification tasks.\n",
    "# set device=-1 if you don't have a gpu\n",
    "pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    framework=\"pt\",\n",
    "    device=0,\n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b800e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on example\n",
    "\n",
    "example = [\n",
    "    \"The patient fell out of bed and it was a severe incident\",\n",
    "    \"the patient fell out of bed and it was all okay\",\n",
    "]\n",
    "pipe(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d2e6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = [\n",
    "    \"The patient fell out of bed and broke their femur\",\n",
    "    \"the patient fell out of bed and was helped back up by a nurse\",\n",
    "]\n",
    "pipe(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95285f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = [\n",
    "    \"pt fell out of bed and broke their femur\",\n",
    "    \"pt fell out of bed and was helped back up by a nurse\",\n",
    "]\n",
    "pipe(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8e877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = [\n",
    "    \"p2 fell out of bed and broke their femur\",\n",
    "    \"p2 fell out of bed and was helped back up by a nurse\",\n",
    "]\n",
    "pipe(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed8c386",
   "metadata": {},
   "source": [
    "Below is taken from the sentiment analysis provided by CheckList example and will highlight the general workflow of using checkpoint when you have an obvious output objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd340e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i : i + n]\n",
    "\n",
    "\n",
    "def batch_predict(pipe, data, batch_size=999999):\n",
    "    ret = []\n",
    "    print(f\"Data before chunks\")\n",
    "    for d in chunks(data, batch_size):\n",
    "        print(f\"d in chunks: {d}\")\n",
    "        ret.extend(pipe(d))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af4796f",
   "metadata": {},
   "source": [
    "The sentiment example adapts a binary classificaiton model to produce output probabilties for 3 classes by assigning any probas in range 0.33* -> 0.66* to a ***neutral*** class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a563062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_and_conf(data):\n",
    "\n",
    "    \"\"\"\n",
    "    Wrapper around the pipe class to return probabilty scores for both the 0 and 1\n",
    "    classes of a binary classification problem\n",
    "    \"\"\"\n",
    "\n",
    "    # print(f\"data is: {data}\")\n",
    "    raw_preds = pipe(data)\n",
    "    preds = np.array([int(p[\"label\"][-1]) for p in raw_preds])\n",
    "    pp = np.array(\n",
    "        [\n",
    "            [p[\"score\"], 1 - p[\"score\"]]\n",
    "            if int(p[\"label\"][-1]) == 0\n",
    "            else [1 - p[\"score\"], p[\"score\"]]\n",
    "            for p in raw_preds\n",
    "        ]\n",
    "    )\n",
    "    return preds, pp\n",
    "\n",
    "\n",
    "def pred_and_conf_neutral(data):\n",
    "    \"\"\"\n",
    "    Wrapper around the pipe class to adapt a model trained on a binary classification\n",
    "    problem to return a neutral class, which will be applied when the probability is\n",
    "    in the range of 0.33-0.66\n",
    "    \"\"\"\n",
    "    # print(f\"Data is : {data}\")\n",
    "    # change format to softmax, make everything in [0.33, 0.66] range be predicted\n",
    "    # as neutral\n",
    "    preds = batch_predict(pipe, data)\n",
    "    # print(f\"Preds is: {preds}\")\n",
    "    pr = np.array(\n",
    "        [x[\"score\"] if x[\"label\"] == \"LABEL_1\" else 1 - x[\"score\"] for x in preds]\n",
    "    )\n",
    "    # print(f\"Pr is: {pr} with shape: {pr.shape}\")\n",
    "    pp = np.zeros((pr.shape[0], 3))\n",
    "    margin_neutral = 1 / 3.0\n",
    "    mn = margin_neutral / 2.0\n",
    "    neg = pr < 0.5 - mn\n",
    "    pp[neg, 0] = 1 - pr[neg]\n",
    "    pp[neg, 2] = pr[neg]\n",
    "    pos = pr > 0.5 + mn\n",
    "    pp[pos, 0] = 1 - pr[pos]\n",
    "    pp[pos, 2] = pr[pos]\n",
    "    neutral_pos = (pr >= 0.5) * (pr < 0.5 + mn)\n",
    "    pp[neutral_pos, 1] = 1 - (1 / margin_neutral) * np.abs(pr[neutral_pos] - 0.5)\n",
    "    pp[neutral_pos, 2] = 1 - pp[neutral_pos, 1]\n",
    "    neutral_neg = (pr < 0.5) * (pr > 0.5 - mn)\n",
    "    pp[neutral_neg, 1] = 1 - (1 / margin_neutral) * np.abs(pr[neutral_neg] - 0.5)\n",
    "    pp[neutral_neg, 0] = 1 - pp[neutral_neg, 1]\n",
    "\n",
    "    new_preds = np.argmax(pp, axis=1)\n",
    "\n",
    "    return new_preds, pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713e8f09",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Below is an example using CheckLists's Editor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1128af0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate checklist editor\n",
    "editor = Editor()\n",
    "\n",
    "# instantiate a test Suite to add to\n",
    "suite = TestSuite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e72fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's find some positive and negative adjectives\n",
    "\", \".join(\n",
    "    editor.suggest(\n",
    "        \"This is not {a:mask} {thing}.\", thing=[\"book\", \"movie\", \"show\", \"game\"]\n",
    "    )[:30]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2b806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [\"good\", \"enjoyable\", \"exciting\", \"excellent\", \"amazing\", \"great\", \"engaging\"]\n",
    "neg = [\"bad\", \"terrible\", \"awful\", \"horrible\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d3bd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = editor.template(\n",
    "    \"This is not {a:pos} {mask}.\", pos=pos, labels=0, save=True, nsamples=100\n",
    ")\n",
    "ret += editor.template(\n",
    "    \"This is not {a:neg} {mask}.\", neg=neg, labels=1, save=True, nsamples=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32895fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6754d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a MFT test object\n",
    "test = MFT(\n",
    "    ret.data,\n",
    "    labels=ret.labels,\n",
    "    name=\"Simple negation\",\n",
    "    capability=\"Negation\",\n",
    "    description=\"Very simple negations.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11659449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can use the test to run the pred func and get results\n",
    "test.run(pred_and_conf, n=100, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa03c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ddc51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.visual_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de08832f",
   "metadata": {},
   "source": [
    "## Incident report - severity classification\n",
    "\n",
    "We can apply similar ideas to the incident severity prediciton task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dbac80",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Minimal Functionality Test\n",
    "\n",
    "MFT is designed to test a particular aspect or task of the model, such as the models ability to handle negation. This was quite easy with the sentiment model, but a little more difficult with incident reports...\n",
    "\n",
    "In this example we are providing data which is very trivial and arguably silly, but the idea is that given a positive adjective with negation should lead to a label of negative or low severity in our case: and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcc2e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate checklist editor\n",
    "editor = Editor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00a68b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's find some positive and negative adjectives\n",
    "\", \".join(\n",
    "    editor.suggest(\n",
    "        \"Patient was {thing} which was a {a:mask} \",\n",
    "        thing=[\"walking\", \"running\", \"waiting\", \"acting\"],\n",
    "    )[:30]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d9e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_severe_tags = [\n",
    "    \"good\",\n",
    "    \"enjoyable\",\n",
    "    \"exciting\",\n",
    "    \"excellent\",\n",
    "    \"amazing\",\n",
    "    \"great\",\n",
    "    \"engaging\",\n",
    "    \"healthy\",\n",
    "    \"appropriate\",\n",
    "]\n",
    "severe_tags = [\n",
    "    \"bad\",\n",
    "    \"terrible\",\n",
    "    \"awful\",\n",
    "    \"horrible\",\n",
    "    \"risky\",\n",
    "    \"breach\",\n",
    "    \"dangerous\",\n",
    "    \"unhealthy\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338474d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = editor.template(\n",
    "    \"This is not {a:pos} {mask}.\",\n",
    "    pos=not_severe_tags,\n",
    "    labels=1,\n",
    "    save=True,\n",
    "    nsamples=100,\n",
    ")\n",
    "ret += editor.template(\n",
    "    \"This is not {a:neg} {mask}.\", neg=severe_tags, labels=0, save=True, nsamples=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9660f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a MFT test object\n",
    "test = MFT(\n",
    "    ret.data,\n",
    "    labels=ret.labels,\n",
    "    name=\"Simple negation\",\n",
    "    capability=\"Negation\",\n",
    "    description=\"Very simple negations.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592a887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to suite\n",
    "description = \"Add negation and expect a change in prediction\"\n",
    "suite.add(\n",
    "    test, \"Add negation to change sentiment\", \"Vocabulary\", description, overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3edd274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can use the test to run the pred func and get results\n",
    "test.run(pred_and_conf, n=100, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31345160",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01524860",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.visual_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2756da15",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Invariance Tests (IV's)\n",
    "\n",
    "Here we want to explore whether changing reltaively trivial parts of a report lead to a difference in the resultant predictions. The main point is that we **do not** expect the models prediction to change!\n",
    "\n",
    " For example we can try:\n",
    "\n",
    " **punctuation | typos | synonyms**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7345448d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Changing gender "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e92c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    \"she complained of chest pains\",\n",
    "    \"she had a bp of 160/100\",\n",
    "    \"she was aggressive towards staff and threatened to hurt them\",\n",
    "    \"he did not receive the medical attention quickly\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5c30cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_genders(x, *args, **kwargs):\n",
    "    # Returns empty or a list of strings with profesions changed\n",
    "    gender_pronouns = [\"she\", \"he\"]\n",
    "    ret = []\n",
    "    for p in gender_pronouns:\n",
    "        if re.search(r\"\\b%s\\b\" % p, x):\n",
    "            ret.extend(\n",
    "                [re.sub(r\"\\b%s\\b\" % p, p2, x) for p2 in gender_pronouns if p != p2]\n",
    "            )\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf41b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_genders(data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ddc96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = Perturb.perturb(data, change_genders, keep_original=True)\n",
    "# ret.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a87bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to run a test with a pertubation method\n",
    "def test_invariant(data: list, method: callable, predict_fn: callable):\n",
    "    t = Perturb.perturb(data, method)\n",
    "    print(f\"\\n\".join(t.data[0]))\n",
    "    print(f\"\\nSummary:\")\n",
    "    test = INV(**t)\n",
    "    test.run(predict_fn, overwrite=True)\n",
    "    # test.summary()\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b4f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_results = test_invariant(data, change_genders, pred_and_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5a4921",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_results.visual_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722077bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to suite\n",
    "description = \"Change gender pronoun\"\n",
    "suite.add(gender_results, \"Change gender pronoun\", \"Vocabulary\", description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1044c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "suite.tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97fbc56",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Adding or removal of punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc1e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    \"patient fell out of bed inside the ward and was fine\",\n",
    "    \"patient developed a sore on buttock\",\n",
    "    \"patient had grade 3 moisture lesion on the sacral area!\",\n",
    "    (\n",
    "        \"patient was walking from toilet to the bed without assistance, the floor was \"\n",
    "        \"wet and patient slipped and hit their head!\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b044fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdata = list(nlp.pipe(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1820ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_results = test_invariant(pdata, Perturb.punctuation, pred_and_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f546ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_results.visual_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192a890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to suite\n",
    "description = \"Add or removal of punctuation and expect no change to prediction\"\n",
    "suite.add(punctuation_results, \"Change punctuation\", \"Vocabulary\", description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb78c2f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4682a6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "typo_results = test_invariant(data, Perturb.add_typos, pred_and_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9dd84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "typo_results.visual_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c79ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to suite\n",
    "description = \"Introducing typos and expect no change\"\n",
    "suite.add(typo_results, \"introduce typos\", \"Vocabulary\", description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a1c905",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Change location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1f710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    \"patient from Leeds fell out of bed inside the ward and was fine\",\n",
    "    \"patient from Leeds developed a sore on buttock\",\n",
    "    \"patient from Leeds had grade 3 moisture lesion on the sacral area!\",\n",
    "    (\n",
    "        \"patient from Leeds was walking from toilet to the bed without assistance, \"\n",
    "        \"the floor was wet and patient slipped and hit their head!\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a54b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to convert to spacy object to leverage location capabiltiies\n",
    "pdata = list(nlp.pipe(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c087e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96309d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_results = test_invariant(pdata, Perturb.change_location, pred_and_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188d9433",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6239bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_results.visual_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf072b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to suite\n",
    "description = \"Change location/country and expect no change\"\n",
    "suite.add(location_results, \"change country\", \"Vocabulary\", description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2275c24",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Change first noun found in sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688a83dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_noun(text: str):\n",
    "    spacy_text = nlp(text)\n",
    "    nouns = [word.text for word in spacy_text if word.tag_ == \"NN\"]\n",
    "    if len(nouns) >= 1:\n",
    "        return nouns[0]\n",
    "    return nouns\n",
    "\n",
    "\n",
    "def change_to_related_nouns(sent: str, num_words: int = 5):\n",
    "    print(f\"sent in: {sent}\")\n",
    "    noun = find_first_noun(sent)\n",
    "    if noun:\n",
    "        print(f\"nouns are: {noun} which has type: {type(noun)}\")\n",
    "        related_nouns = editor.related_words(sent, noun)[:num_words]\n",
    "        return [sent.replace(noun, new_word) for new_word in related_nouns]\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22389b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_to_related_nouns(\"the bed is very small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8cc579",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_results = test_invariant(data, change_to_related_nouns, pred_and_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6474595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_results.visual_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to suite\n",
    "description = \"Change related nouns and expect no change\"\n",
    "suite.add(noun_results, \"change nouns\", \"Vocabulary\", description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0a7359",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Examples where patient is replaced with pt | p1 | p2 | patient1 | patient2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a558e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    \"patient fell out of bed and was fine\",\n",
    "    \"patient developed a sore on buttock\",\n",
    "    \"patient had grade 3 moisture lesion on the sacral area\",\n",
    "    \"p1 was walking between the bathroom and the ward and slipped on a wet floor\",\n",
    "    \"p2 attacked p1 after a verbal altercation and security had to be called\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df96d235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_patient_noun(x, *args, **kwargs):\n",
    "    # Returns empty or a list of strings with the patient noun changed\n",
    "    patient_nouns = [\"patient\", \"p1\", \"p2\", \"pt\"]\n",
    "    ret = []\n",
    "    for p in patient_nouns:\n",
    "        if re.search(r\"\\b%s\\b\" % p, x):\n",
    "            ret.extend(\n",
    "                [re.sub(r\"\\b%s\\b\" % p, p2, x) for p2 in patient_nouns if p != p2]\n",
    "            )\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc885983",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2d5d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_patient_noun(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce4905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_noun_results = test_invariant(data, change_patient_noun, pred_and_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e831bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_noun_results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081ab494",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_noun_results.visual_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88453ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to suite\n",
    "description = \"Change patient noun and expect no change\"\n",
    "suite.add(patient_noun_results, \"change patient noun\", \"Vocabulary\", description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089afc9c",
   "metadata": {},
   "source": [
    "### Directional Expectation test \n",
    "\n",
    "Whilst in invariance testing we expect the models ouputs to be the same before and after perturbation, with DE's we expect changes to prediction. Such as the negation - here we can use examples that we know the model will assign a label of 1 for severe and see if the probabilities go up when adding negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d11c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    \"patient fell out of bed and was hurt\",\n",
    "    \"patient did develop a sore on buttock\",\n",
    "    \"patient had grade 3 moisture lesion on the sacral area\",\n",
    "]\n",
    "# need to convert to spacy object to leverage location capabiltiies\n",
    "pdata = list(nlp.pipe(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086d2be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a607632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add negation\n",
    "# NOTE this is an experimental feature of checklist which utilises spacy to try\n",
    "# determine where negation can be added - is very prone to not working as desired\n",
    "t = Perturb.perturb(pdata, Perturb.add_negation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb83a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279f5159",
   "metadata": {},
   "source": [
    "What would we expect after this perturbation? I think the least we should expect is that the prediction probability of positive should **not go up** (that is, it should be monotonically decreasing).\n",
    "\n",
    "Monotonicity is an expectation function that is built in, so we don't need to implement it.\n",
    "`tolerance=0.1` means we won't consider it a failure if the prediction probability goes up by less than 0.1, only if it goes up by more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f225188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.expect import Expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37893327",
   "metadata": {},
   "outputs": [],
   "source": [
    "monotonic_decreasing = Expect.monotonic(label=1, increasing=False, tolerance=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb09880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just use the DIR class this time\n",
    "test = DIR(**t, expect=monotonic_decreasing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0736c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.run(pred_and_conf, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4f87f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ed934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.visual_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b700f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to suite\n",
    "description = (\n",
    "    \"Auto negation DIR test monotonic decrease. Here we are adding negation to \"\n",
    "    \"potentially severe incidents and expecting that the probabilties for class 1 \"\n",
    "    \"do not go up\"\n",
    ")\n",
    "suite.add(test, \"negation monotonic decrease\", \"Vocabulary\", description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a957cf",
   "metadata": {},
   "source": [
    "Save suite and reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297caffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "suite.tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a523158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./test_suites/\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "suite.save(f\"{path}/severity_suite.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823ecf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test reloading\n",
    "reloaded_suite = TestSuite.from_file(f\"{path}/severity_suite.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd6281",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_suite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49be7774",
   "metadata": {},
   "source": [
    "Run all from reloaded suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa5bb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_suite.run(pred_and_conf, n=500, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138a63c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_suite.visual_summary_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3864a8a",
   "metadata": {},
   "source": [
    "### Take a look at real examples that models are sure/unsure about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6ee070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for all data\n",
    "data_preds = pred_and_conf(df.text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aedde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds_probs(\n",
    "    data: pd.DataFrame,\n",
    "    pred_fn: callable,\n",
    "    text_col: str = \"text\",\n",
    "    label_col: str = \"label\",\n",
    "):\n",
    "\n",
    "    # get all the predictions and probabilities\n",
    "    all_preds_probs = pred_fn(data[text_col].tolist())\n",
    "\n",
    "    # the first element should be all labels in order and 2nd should be all probs\n",
    "    # class indexed\n",
    "\n",
    "    # add to dataframe\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"text\": data[text_col].tolist(),\n",
    "            \"label\": data[label_col],\n",
    "            \"prediction\": all_preds_probs[0],\n",
    "            \"label_0_proba\": all_preds_probs[1][:, 0],\n",
    "            \"label_1_proba\": all_preds_probs[1][:, 1],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e669d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "elm4psir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:53:40) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "036f9b356400688fa32ab139d64151f7af42c87240ca002d464048bf8c685a85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
