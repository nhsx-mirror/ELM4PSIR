{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce0d4c6d",
   "metadata": {
    "_uuid": "84163f3ca19c0b7c9fda47121b3bc4cadfaf1fcc"
   },
   "source": [
    "# Gensim Word2VecÂ Tutorial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3f382d1",
   "metadata": {
    "_uuid": "9e858d8c6e56ee78629e1cc21ce8fee6dc3cec0b"
   },
   "source": [
    "Gensim tutorial for biomedical word2vec model training and playing around\n",
    "\n",
    "Based on <http://mccormickml.com/2016/04/27/word2vec-resources/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ed2607",
   "metadata": {
    "_uuid": "aa10f4823faa729e96f15bb865e9f20769ea7a4b"
   },
   "source": [
    "# Plan\n",
    "\n",
    "1. [Briefing about Word2Vec](#Briefing-about-Word2Vec:)\n",
    "    * [Purpose of the tutorial](#Purpose-of-the-tutorial:)\n",
    "    * [Brief explanation](#Brief-explanation:)\n",
    "\n",
    "2. [Getting Started](#Getting-Started)\n",
    "    * [Setting up the environment](#Setting-up-the-environment:)\n",
    "    * [The data](#The-data:)\n",
    "3. [Preprocessing](#Preprocessing)\n",
    "    * [Cleaning](#Cleaning)\n",
    "    * [Bigrams](#Bigrams)\n",
    "    * [Most frequent words](#Most-Frequent-Words)\n",
    "    \n",
    "4. [Training the Model](#Training-the-model)\n",
    "    * [Gensim Word2Vec Implementation](#Gensim-Word2Vec-Implementation:)\n",
    "    * [Why I seperate the training of the model in 3 steps](#Why-I-seperate-the-training-of-the-model-in-3-steps:)\n",
    "    * [Training the model](#Training-the-model)\n",
    "        * [The parameters](#The-parameters)\n",
    "        * [Building the vocabulary table](#Building-the-Vocabulary-Table)\n",
    "        * [Training of the model](#Training-of-the-model)\n",
    "        * [Saving the model](#Saving-the-model:)\n",
    "5. [Exploring the Model](#Exploring-the-model)\n",
    "    * [Most similar to](#Most-similar-to:)\n",
    "    * [Similarities](#Similarities:)\n",
    "    * [Odd-one-out](#Odd-One-Out:)\n",
    "    * [Analogy difference](#Analogy-difference:)\n",
    "    * [t-SNE visualizations](#t-SNE-visualizations:)\n",
    "        * [10 Most similar words vs. 8 Random words](#10-Most-similar-words-vs.-8-Random-words:)\n",
    "        * [10 Most similar words vs. 10 Most dissimilar](#10-Most-similar-words-vs.-10-Most-dissimilar:)\n",
    "        * [10 Most similar words vs. 11th to 20th Most similar words](#10-Most-similar-words-vs.-11th-to-20th-Most-similar-words:)\n",
    "6. [Final Thoughts](#Final-Thoughts)\n",
    "7. [Material for more in depths understanding](#Material-for-more-in-depths-understanding:)\n",
    "8. [Acknowledgements](#Acknowledgements)\n",
    "9. [References](#References:)\n",
    "10. [End](#End)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da46ed9d",
   "metadata": {
    "_uuid": "7d96105f0c90bf052b2afdb684bf31549e1e6c81"
   },
   "source": [
    "# Briefing about Word2Vec:\n",
    "\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png\" alt=\"drawing\" width=\"550\"/>\n",
    "\n",
    "[[1]](#References:)\n",
    "\n",
    "\n",
    "## Purpose of the tutorial:\n",
    "As I said before, this tutorial focuses on the right use of the Word2Vec package from the Gensim libray; therefore, I am not going to explain the concepts and ideas behind Word2Vec here. I am simply going to give a very brief explanation, and provide you with links to good, in depth tutorials.\n",
    "\n",
    "## Brief explanation:\n",
    "\n",
    "Word2Vec was introduced in two [papers](#Material-for-more-in-depths-understanding:) between September and October 2013, by a team of researchers at Google. Along with the papers, the researchers published their implementation in C. The Python implementation was done soon after the 1st paper, by [Gensim](https://radimrehurek.com/gensim/index.html). \n",
    "\n",
    "The underlying assumption of Word2Vec is that two words sharing similar contexts also share a similar meaning and consequently a similar vector representation from the model. For instance: \"dog\", \"puppy\" and \"pup\" are often used in similar situations, with similar surrounding words like \"good\", \"fluffy\" or \"cute\", and according to Word2Vec they will therefore share a similar vector representation.<br>\n",
    "\n",
    "From this assumption, Word2Vec can be used to find out the relations between words in a dataset, compute the similarity between them, or use the vector representation of those words as input for other applications such as text classification or clustering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54971f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3611d26e",
   "metadata": {
    "_uuid": "cc7b3e6ca62670ff13626705402f626778487204"
   },
   "outputs": [],
   "source": [
    "import logging  # Setting up the loggings to monitor gensim\n",
    "import re  # For preprocessing\n",
    "from collections import defaultdict  # For word frequency\n",
    "from time import time  # To time our operations\n",
    "\n",
    "import pandas as pd  # For data handling\n",
    "import spacy  # For preprocessing\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(levelname)s - %(asctime)s: %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f7aaa",
   "metadata": {
    "_uuid": "0c36323d9aa62f74ab348cda5ee0f571aa1d4a96"
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d54ad14",
   "metadata": {
    "_uuid": "6453b9c3f797e51923e030090ead659253f4e459"
   },
   "outputs": [],
   "source": [
    "# data dir for the text data\n",
    "data_dir = \"<DATA_DIR>\"\n",
    "## we just want text so lets grab the text column only\n",
    "df = pd.read_csv(f\"{data_dir}\", sep=\"delimiter\", header=None)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe97a6c2",
   "metadata": {
    "_uuid": "c6c6bf4462fb4bc00c2abdbf65eced888219f364"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05578abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with dataframes is easier if we have a nicer column name - so rename 0 to TEXT\n",
    "df.rename(columns={0: \"TEXT\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea0359a",
   "metadata": {
    "_uuid": "3a15727caeba1c8d10573456640d0b8b9f2f2e2d"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d29fae1",
   "metadata": {
    "_uuid": "885a555596d7484841ea54c94405d03d90572396"
   },
   "source": [
    "Removing the missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b6f60a",
   "metadata": {
    "_uuid": "82cb38f176526679f66ee31e11cfe4f5eebdab51"
   },
   "outputs": [],
   "source": [
    "df = df.dropna().reset_index(drop=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20873fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d354de",
   "metadata": {
    "_uuid": "7f07dca2a2656dcd9e0c315afa36af32a992eef7"
   },
   "source": [
    "## Cleaning:\n",
    "We are lemmatizing and removing the stopwords and non-alphabetic characters for each line of dialogue. \n",
    "\n",
    "### NOTE\n",
    "Lemmatizing is probably something we would skip for medical texts usually - it can often reduce the vocabularly substantially and may not work so well for niche medical terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb905169",
   "metadata": {
    "_uuid": "b26a0c01c5701630d3951cfc808a9d944eea6371"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\n",
    "    \"en_core_web_sm\", disable=[\"ner\", \"parser\"]\n",
    ")  # disabling Named Entity Recognition for speed\n",
    "\n",
    "\n",
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    if len(txt) > 2:\n",
    "        return \" \".join(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f75437b",
   "metadata": {
    "_uuid": "f686964722eede40e5961cd232aee7b6dd587bd1"
   },
   "source": [
    "Removes non-alphabetic characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dc44a8",
   "metadata": {
    "_uuid": "b45598934171607242ca7d50f8c5f7c91411aace"
   },
   "outputs": [],
   "source": [
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", \" \", str(row)).lower() for row in df[\"TEXT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8584ba1c",
   "metadata": {
    "_uuid": "2360160a7f326a32d56f2f18782d7ce2f4ac1def"
   },
   "source": [
    "Taking advantage of spaCy .pipe() attribute to speed-up the cleaning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e204aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, n_process=2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10361488",
   "metadata": {
    "_uuid": "65ec76c60f9fe93bba6909f4e696f90e3e54710b"
   },
   "source": [
    "Put the results in a DataFrame to remove missing values and duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bdb04d",
   "metadata": {
    "_uuid": "57f1eb8382554bc592d48915a903230b5b6d6cf7"
   },
   "outputs": [],
   "source": [
    "df_clean = pd.DataFrame({\"clean\": txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5301b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08745e6e",
   "metadata": {
    "_uuid": "31b4a744059df490ddb47ab6cdec008dc929ede3"
   },
   "source": [
    "## Bigrams:\n",
    "We are using Gensim Phrases package to automatically detect common phrases (bigrams) from a list of sentences.\n",
    "https://radimrehurek.com/gensim/models/phrases.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d884df",
   "metadata": {
    "_uuid": "af6d420284a0ff7a7407d4c526754ffe850d6170"
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phraser, Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711acbd2",
   "metadata": {
    "_uuid": "788aec3c82788101db25d4ca6105ee133fecae7c"
   },
   "source": [
    "As `Phrases()` takes a list of list of words as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8106c8b9",
   "metadata": {
    "_uuid": "f58487ff08d8812622fd7aef36139f1c850add18"
   },
   "outputs": [],
   "source": [
    "sent = [row.split() for row in df_clean[\"clean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee38dbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d6dc8c",
   "metadata": {
    "_uuid": "bb7766b322cbc1d3381912b890585eb249ac5304"
   },
   "source": [
    "Creates the relevant phrases from the list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca6db21",
   "metadata": {
    "_uuid": "8befad8c76c54bd2b831b0942a2f626f7d8a6dac"
   },
   "outputs": [],
   "source": [
    "phrases = Phrases(sent, min_count=30, progress_per=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad416ee7",
   "metadata": {
    "_uuid": "45bae4a953f2ad8951e4efb234e1e357857a33b3"
   },
   "source": [
    "The goal of Phraser() is to cut down memory consumption of Phrases(), by discarding model state not strictly needed for the bigram detection task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df6a9de",
   "metadata": {
    "_uuid": "b8ae81ba230013aefe7c584338de7376fedf6294",
    "incorrectly_encoded_metadata": "_kg_hide-input=true"
   },
   "outputs": [],
   "source": [
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ce0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d54b914",
   "metadata": {
    "_uuid": "4a58380f19d159688aeee665d1afb96289fdd4b8"
   },
   "source": [
    "Transform the corpus based on the bigrams detected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4344df2a",
   "metadata": {
    "_uuid": "8051b56890c147119db3df529d3cfd3cf675fdca"
   },
   "outputs": [],
   "source": [
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3674bb",
   "metadata": {
    "_uuid": "a4f81e8bb2c09a67b00cd24db28353eca8ae188c"
   },
   "source": [
    "## Most Frequent Words:\n",
    "Mainly a sanity check of the effectiveness of the lemmatization, removal of stopwords, and addition of bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99eb17d",
   "metadata": {
    "_uuid": "eeb8afe1cfcb7ba65bd14d657455600acacf39ba"
   },
   "outputs": [],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e5ca6",
   "metadata": {
    "_uuid": "5b010149150b2b2eaf332d79bcde0649b8a3c2b5"
   },
   "outputs": [],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9591284a",
   "metadata": {
    "_uuid": "500ab7b5c84dc006d7945f339c40725a82856fdf"
   },
   "source": [
    "# Training the model\n",
    "## Gensim Word2Vec Implementation:\n",
    "We use Gensim implementation of word2vec: https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e438964a",
   "metadata": {
    "_uuid": "3269be205cadbad499aa87890893d92da6adc796"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e7f573",
   "metadata": {
    "_uuid": "7c524bc49c41a6c37f9e754a38797c9501202090"
   },
   "source": [
    "## Why I seperate the training of the model in 3 steps:\n",
    "I prefer to separate the training in 3 distinctive steps for clarity and monitoring.\n",
    "1. `Word2Vec()`: \n",
    ">In this first step, I set up the parameters of the model one-by-one. <br>I do not supply the parameter `sentences`, and therefore leave the model uninitialized, purposefully.\n",
    "2. `.build_vocab()`: \n",
    ">Here it builds the vocabulary from a sequence of sentences and thus initialized the model. <br>With the loggings, I can follow the progress and even more important, the effect of `min_count` and `sample` on the word corpus. I noticed that these two parameters, and in particular `sample`, have a great influence over the performance of a model. Displaying both allows for a more accurate and an easier management of their influence.\n",
    "3. `.train()`:\n",
    ">Finally, trains the model.<br>\n",
    "The loggings here are mainly useful for monitoring, making sure that no threads are executed instantaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0510c3",
   "metadata": {
    "_uuid": "03488d9b68963579c96094aca88a302c9f2753a7"
   },
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()  # Count the number of cores in a computer\n",
    "\n",
    "cores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08473fb",
   "metadata": {
    "_uuid": "89c305fcd163488441ac2ac6133678bd973b4419"
   },
   "source": [
    "## The parameters:\n",
    "\n",
    "* `min_count` <font color='purple'>=</font> <font color='green'>int</font> - Ignores all words with total absolute frequency lower than this - (2, 100)\n",
    "\n",
    "\n",
    "* `window` <font color='purple'>=</font> <font color='green'>int</font> - The maximum distance between the current and predicted word within a sentence. E.g. `window` words on the left and `window` words on the left of our target - (2, 10)\n",
    "\n",
    "\n",
    "* `size` <font color='purple'>=</font> <font color='green'>int</font> - Dimensionality of the feature vectors. - (50, 300)\n",
    "\n",
    "\n",
    "* `sample` <font color='purple'>=</font> <font color='green'>float</font> - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial.  - (0, 1e-5)\n",
    "\n",
    "\n",
    "* `alpha` <font color='purple'>=</font> <font color='green'>float</font> - The initial learning rate - (0.01, 0.05)\n",
    "\n",
    "\n",
    "* `min_alpha` <font color='purple'>=</font> <font color='green'>float</font> - Learning rate will linearly drop to `min_alpha` as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n",
    "\n",
    "\n",
    "* `negative` <font color='purple'>=</font> <font color='green'>int</font> - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n",
    "\n",
    "\n",
    "* `workers` <font color='purple'>=</font> <font color='green'>int</font> - Use these many worker threads to train the model (=faster training with multicore machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9f4e11",
   "metadata": {
    "_uuid": "ad619db82c219d6cb81fad516563feb0c4d474cd"
   },
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(\n",
    "    min_count=20,\n",
    "    window=2,\n",
    "    vector_size=300,\n",
    "    sample=6e-5,\n",
    "    alpha=0.03,\n",
    "    min_alpha=0.0007,\n",
    "    negative=20,\n",
    "    workers=cores - 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf32fa93",
   "metadata": {
    "_uuid": "d7e9f1bd338f9e15647b5209ffd8fbb131cd7ee5"
   },
   "source": [
    "## Building the Vocabulary Table:\n",
    "Word2Vec requires us to build the vocabulary table (simply digesting all the words and filtering out the unique words, and doing some basic counts on them):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54c0369",
   "metadata": {
    "_uuid": "66358ad743e05e17dfbed3899af9c41056143daa"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dfed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.sorted_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03344e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.corpus_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec1d9ed",
   "metadata": {
    "_uuid": "63260d82061abb47db7f2f8b23e07ec629adf5a9"
   },
   "source": [
    "## Training of the model:\n",
    "_Parameters of the training:_\n",
    "* `total_examples` <font color='purple'>=</font> <font color='green'>int</font> - Count of sentences;\n",
    "* `epochs` <font color='purple'>=</font> <font color='green'>int</font> - Number of iterations (epochs) over the corpus - [10, 20, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf1f9d7",
   "metadata": {
    "_uuid": "07a2a047e701e512fd758edff186daadaeea6461"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "w2v_model.train(\n",
    "    sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd860195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "save_path = \"<SAVE_PATH>\"\n",
    "w2v_model.save(f\"{save_path}/incident_word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc63b348",
   "metadata": {
    "_uuid": "a420d5a98eb860cff1f4bbac8cbe2054459b6200"
   },
   "source": [
    "# Exploring the model\n",
    "## Most similar to:\n",
    "\n",
    "Here, we will ask our model to find the word most similar to some basic medical concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30166f80",
   "metadata": {
    "_uuid": "339207a733a1ac42fe60e32a29f9e5d5ca0a9275"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"cardiac\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508385e4",
   "metadata": {
    "_uuid": "3b6686e6fa956a98450259b063b4cf51019a6d0b"
   },
   "source": [
    "_A small precision here:_<br>\n",
    "Can try some bigrams too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d56592c",
   "metadata": {
    "_uuid": "23e5149b19f18f4f2f456d4c72afc5c188bcfba4"
   },
   "outputs": [],
   "source": [
    "# w2v_model.wv.most_similar(positive=[\"heart_disease\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c5371c",
   "metadata": {
    "_uuid": "22595f98c675a9697243b7e826b2840e5fc3e5f5"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"lung\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749126aa",
   "metadata": {
    "_uuid": "d8b5937dfd7584f168a33060c435036cad5b390b"
   },
   "source": [
    "## Similarities:\n",
    "Here, we will see how similar are two words to each other :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e08ad0",
   "metadata": {
    "_uuid": "367755f5c9e00de4bc5056c978f5b50a38c1368b"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity(\"leg\", \"head\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4405bcd8",
   "metadata": {},
   "source": [
    "# Odd-One-Out\n",
    "\n",
    "Here we can request the model provides the word that does not belong to a given list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad03bf",
   "metadata": {
    "_uuid": "d982e44d9c212b5ee09bcaebd050a725ab5e508e"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.doesnt_match([\"discharge\", \"heart\", \"lung\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697512db",
   "metadata": {
    "_uuid": "df95cdff693e843ab4b4c174fea24029447573cd"
   },
   "source": [
    "## Analogy difference:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497807dc",
   "metadata": {
    "_uuid": "812961e79dde9f2032f708755ca287c0aef838d0"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"heart\", \"lung\"], negative=[\"leg\"], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0976b27a",
   "metadata": {
    "_uuid": "773c0acc8750ba8e728ff261f2e9ec39694c245c"
   },
   "source": [
    "### t-SNE visualizations:\n",
    "t-SNE is a non-linear dimensionality reduction algorithm that attempts to represent high-dimensional data and the underlying relationships between vectors in a lower-dimensional space.<br>\n",
    "Here is a good tutorial on it: https://medium.com/@luckylwk/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4816cac6",
   "metadata": {
    "_uuid": "27ec46110042fc28da900b1b344ae4e0692d5dc2"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c7d1e9",
   "metadata": {
    "_uuid": "22693eaa25253b38cee3c5cd5db6b6fdddb575a4"
   },
   "source": [
    "Our goal in this section is to plot our 300 dimensions vectors into 2 dimensional graphs, and see if we can spot interesting patterns.<br>\n",
    "For that we are going to use t-SNE implementation from scikit-learn.\n",
    "\n",
    "To make the visualizations more relevant, we will look at the relationships between a query word (in <font color='red'>**red**</font>), its most similar words in the model (in <font color=\"blue\">**blue**</font>), and other words from the vocabulary (in <font color='green'>**green**</font>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd3eaf9",
   "metadata": {
    "_uuid": "489a7d160dcd92da0ce42a3b5b461368c9ffe5f1"
   },
   "outputs": [],
   "source": [
    "def tsnescatterplot(model, word, list_names, features_dimensionality=300):\n",
    "    \"\"\"Plot in seaborn the results from the t-SNE dimensionality reduction algorithm\n",
    "    of the vectors of a query word,\n",
    "    its list of most similar words, and a list of words.\n",
    "    \"\"\"\n",
    "    arrays = np.empty((0, features_dimensionality), dtype=\"f\")\n",
    "    word_labels = [word]\n",
    "    color_list = [\"red\"]\n",
    "\n",
    "    # adds the vector of the query word\n",
    "    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n",
    "\n",
    "    # gets list of most similar words\n",
    "    close_words = model.wv.most_similar([word])\n",
    "\n",
    "    print(f\"length of close words: {len(close_words)}\")\n",
    "\n",
    "    # adds the vector for each of the closest words to the array\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append(\"blue\")\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "\n",
    "    # adds the vector for each of the words from list_names to the array\n",
    "    for wrd in list_names:\n",
    "        wrd_vector = model.wv.__getitem__([wrd])\n",
    "        word_labels.append(wrd)\n",
    "        color_list.append(\"green\")\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "\n",
    "    print(f\"Shape of arrays going to PCA: {arrays.shape}\")\n",
    "    # for PCA the n_components must equal min(n_samples, n_features)\n",
    "\n",
    "    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n",
    "    reduc = PCA(\n",
    "        n_components=min(arrays.shape[0], features_dimensionality)\n",
    "    ).fit_transform(arrays)\n",
    "\n",
    "    # Finds t-SNE coordinates for 2 dimensions\n",
    "    np.set_printoptions(suppress=True)\n",
    "\n",
    "    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n",
    "\n",
    "    # Sets everything up to plot\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"x\": [x for x in Y[:, 0]],\n",
    "            \"y\": [y for y in Y[:, 1]],\n",
    "            \"words\": word_labels,\n",
    "            \"color\": color_list,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    fig, _ = plt.subplots()\n",
    "    fig.set_size_inches(9, 9)\n",
    "\n",
    "    # Basic plot\n",
    "    p1 = sns.regplot(\n",
    "        data=df,\n",
    "        x=\"x\",\n",
    "        y=\"y\",\n",
    "        fit_reg=False,\n",
    "        marker=\"o\",\n",
    "        scatter_kws={\"s\": 40, \"facecolors\": df[\"color\"]},\n",
    "    )\n",
    "\n",
    "    # Adds annotations one by one with a loop\n",
    "    for line in range(0, df.shape[0]):\n",
    "        p1.text(\n",
    "            df[\"x\"][line],\n",
    "            df[\"y\"][line],\n",
    "            \"  \" + df[\"words\"][line].title(),\n",
    "            horizontalalignment=\"left\",\n",
    "            verticalalignment=\"bottom\",\n",
    "            size=\"medium\",\n",
    "            color=df[\"color\"][line],\n",
    "            weight=\"normal\",\n",
    "        ).set_size(15)\n",
    "\n",
    "    plt.xlim(Y[:, 0].min() - 50, Y[:, 0].max() + 50)\n",
    "    plt.ylim(Y[:, 1].min() - 50, Y[:, 1].max() + 50)\n",
    "\n",
    "    plt.title(\"t-SNE visualization for {}\".format(word.title()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5bc6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ca6c7a",
   "metadata": {
    "_uuid": "3943c170a5f5f09974d90c89bdb9ec761e63a416"
   },
   "source": [
    "Code inspired by: [[2]](#References:)\n",
    "\n",
    "## 10 Most similar words vs. 8 Random words:\n",
    "Let's compare where the vector representation of Homer, his 10 most similar words from the model, as well as 8 random ones, lies in a 2D graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f6276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"heart\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c377b",
   "metadata": {
    "_uuid": "18d788b2a92f94771a5f9485a885d44dfba62a94"
   },
   "outputs": [],
   "source": [
    "tsnescatterplot(\n",
    "    w2v_model, word, [\"bedside\", \"left\", \"discharge\", \"give\", \"immediately\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c0c67",
   "metadata": {
    "_uuid": "c73fc2faaf0baecc84f02a97b50cb9ccefa48686"
   },
   "source": [
    "## 10 Most similar words vs. 10 Most dissimilar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b858c94",
   "metadata": {
    "_uuid": "10c77b072f7c281f2be919341be116565c20d8a8"
   },
   "outputs": [],
   "source": [
    "tsnescatterplot(\n",
    "    w2v_model, word, [i[0] for i in w2v_model.wv.most_similar(negative=[word])]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9821c4",
   "metadata": {
    "_uuid": "87315bfbaceb3733bd7af035db6c59cfc4b1ba7f"
   },
   "source": [
    "## 10 Most similar words vs. 11th to 20th Most similar words:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253b3dc6",
   "metadata": {
    "_uuid": "e6f0bc598922f4f2cd17d2511560242a3c35fdd9"
   },
   "outputs": [],
   "source": [
    "tsnescatterplot(\n",
    "    w2v_model,\n",
    "    word,\n",
    "    [t[0] for t in w2v_model.wv.most_similar(positive=[word], topn=20)][10:],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab342603",
   "metadata": {
    "_uuid": "011aa35b717d985f1d9fb820208531222055ff7b"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# Materials for more in depths understanding:\n",
    "* Word Embeddings introduction: https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "* Word2Vec introduction: https://skymind.ai/wiki/word2vec\n",
    "* Another Word2Vec introduction: http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "* A great Gensim implentation tutorial: http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.W467ScBjM2x\n",
    "* Original articles from Mikolov et al.: https://arxiv.org/abs/1301.3781 and https://arxiv.org/abs/1310.4546\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "elm4psir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:53:40) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "036f9b356400688fa32ab139d64151f7af42c87240ca002d464048bf8c685a85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
