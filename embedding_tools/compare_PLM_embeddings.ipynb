{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b82c6341",
   "metadata": {
    "id": "G_WElanyAGHz"
   },
   "source": [
    "__NOTE__ This notebook originated and was produced by Chris McCormick and you can find the original [here](https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial).\n",
    "\n",
    "## A playground for comparing PLMs embeddings\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a229d6d5",
   "metadata": {
    "id": "uXKyKe3NZONV"
   },
   "source": [
    "### Introduction\n",
    "\n",
    "We have added a couple of functions/ideas and removed certain sections throughout, and it may be that the original is easier to follow so please do check it out. But the majority is taken directly from the referenced tutorial. The main goal of this repo is to explore how different BERT/RoBERTa models produce embeddings based on the pre-training and fine-tuning they have recieved. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "210cfef0",
   "metadata": {
    "id": "6Laip2bqmRK-"
   },
   "source": [
    "If your text data is domain specific (e.g. legal, financial, academic, industry-specific) or otherwise different from the \"standard\" text corpus used to train BERT and other langauge models you might want to consider either continuing to train BERT with some of your text data or looking for a domain-specific language model.\n",
    "\n",
    "Faced with the issue mentioned above, a number of researchers have created their own domain-specific language models. These models are created by  training the BERT architecture *from scratch* on a domain-specific corpus rather than the general purpose English text corpus used to train the original BERT model. This leads to a model with vocabulary and word embeddings better suited than the original BERT model to domain-specific NLP problems. Some examples include: \n",
    "\n",
    "- SciBERT (biomedical and computer science literature corpus)\n",
    "- FinBERT (financial services corpus)\n",
    "- BioBERT (biomedical literature corpus)\n",
    "- ClinicalBERT (clinical notes corpus)\n",
    "- mBERT (corpora from multiple languages)\n",
    "- patentBERT (patent corpus)\n",
    "\n",
    "In this notebook, we will explore the embedding space created by differently trained RoBERTa models with a focus on patient safety incident reports.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a2721b",
   "metadata": {
    "id": "ku_4hKSLCaSE"
   },
   "source": [
    "#### Why not do my own pre-training?\n",
    "\n",
    "If you think your text is too domain-specific for the generic BERT, your first thought might be to train BERT from scratch on your own dataset. (Just to be clear: BERT was \"Pre-Trained\" by Google, and we download and \"Fine-Tune\" Google's pre-trained model on our own data. When I say \"train BERT from scratch\", I mean specifically re-doing BERT's *pre-training*).\n",
    "\n",
    "Chances are you won't be able to pre-train BERT on your own dataset, though, for the following reasons. \n",
    "\n",
    "**1. Pre-training BERT requires a huge corpus**\n",
    "\n",
    "BERT-base is a 12-layer neural network with roughly 110 million weights. This enormous size is key to BERT's impressive performance. To train such a complex model, though, (and expect it to work) requires an enormous dataset, on the order of 1B words. Wikipedia is a suitable corpus, for example, with its ~10 million articles. For the majority of applications I assume you won't have a dataset with that many documents. \n",
    "\n",
    "**2. Huge Model + Huge Corpus = Lots of GPUs**\n",
    "\n",
    "Pre-Training BERT is expensive. The cost of pre-training is a whole subject of discussion, and there's been a lot of work done on bringing the cost down, but a *single* pre-training experiment could easily cost you thousands of dollars in GPU or TPU time. \n",
    "\n",
    "That's why these domain-specific pre-trained models are so interesting. Other organizations have footed the bill to produce and share these models which, while not pre-trained on your specific dataset, may at least be much closer to yours than \"generic\" BERT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c42543",
   "metadata": {
    "id": "cFTiBsi8Ti-h"
   },
   "source": [
    "### Example Code for Importing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aba5a4f",
   "metadata": {
    "id": "aJF_caESC0yS"
   },
   "source": [
    "If you're interested in a BERT variant from the community models in the transformers library, importing can be incredibly simple--you just supply the name of the model as it appears in the library page.\n",
    "\n",
    "First, we'll need to install the `transformers` library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59b3588",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "R36LrzeQGDUq",
    "outputId": "7ea1867b-9d24-4b55-9fed-172d8222044e"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from d3blocks import D3Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2753a20",
   "metadata": {
    "id": "DIcEh7LvzMvO"
   },
   "source": [
    "\n",
    "\n",
    "The `transformers` library includes classes for different model architectures (e.g., `AutoModel`, `AlAutoModel`, `RobertaModel`, ...). With whatever model you're using, it needs to be loaded with the correct class (based on its architecture), which may not be immediately apparent. \n",
    "\n",
    "Luckily, the `transformers` library has a solution for this, demonstrated in the following cell. These \"Auto\" classes will choose the correct architecture for you! \n",
    "\n",
    "That's a nice feature, but I'd still prefer to know what I'm working with, so I'm printing out the class names (which show that SciBERT uses the original BERT classes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81afb72",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "oJFsRo_vGDYU",
    "outputId": "070d8a53-86f1-41d1-a680-4705a3129bbc"
   },
   "outputs": [],
   "source": [
    "cache_dir = \".cache\"  # if you want to use a non-default cache directory for downloaded models - change this\n",
    "scibert_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"allenai/scibert_scivocab_uncased\", cache_dir=cache_dir\n",
    ")\n",
    "scibert_model = AutoModel.from_pretrained(\n",
    "    \"allenai/scibert_scivocab_uncased\", cache_dir=cache_dir\n",
    ")\n",
    "\n",
    "print(\"scibert_tokenizer is type:\", type(scibert_tokenizer))\n",
    "print(\"    scibert_model is type:\", type(scibert_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4528e0ce",
   "metadata": {
    "id": "qEVnhVwtV3d5"
   },
   "source": [
    "### Comparing different transformer based PLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e03650d",
   "metadata": {
    "id": "uquAjkdLzg7N"
   },
   "source": [
    "#### Comparing Vocabularies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c38063",
   "metadata": {
    "id": "RwgdQOu_8yrC"
   },
   "source": [
    "The most apparent difference between SciBERT and the original BERT should be the model's vocabulary, since they were trained on such different corpuses.\n",
    "\n",
    "Both tokenizers have a 30,000 word vocabulary that was automatically built based on the most frequently seen words and subword units in their respective corpuses. \n",
    "\n",
    "The authors of SciBERT note:\n",
    "\n",
    "> \"The resulting token overlap between [BERT vocabulary] and\n",
    "[SciBERT vocabulary] is 42%, illustrating a substantial difference in frequently used words between scientific and general domain texts.\"\n",
    "\n",
    "Let's load the original BERT as well and do some of our own comparisons.\n",
    "\n",
    "*Side note: BERT used a \"WordPiece\" model for tokenization, whereas SciBERT employs a newer approach called \"SentencePiece\", but the difference is mostly cosmetic. I cover SentencePiece in more detail in our [ALBERT eBook](https://www.chrismccormick.ai/offers/HaABTJQH).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4e2eed",
   "metadata": {
    "id": "8HyL0YsC8vta"
   },
   "outputs": [],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", cache_dir=cache_dir)\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\", cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dad794",
   "metadata": {
    "id": "lNO_QG48X-0C"
   },
   "source": [
    "Let's apply both tokenizers to some biomedical text and see how they compare. \n",
    "\n",
    "I took the below sentence from the 2001 paper [Hydrogels for biomedical applications](http://yunus.hacettepe.edu.tr/~damlacetin/kmu407/index_dosyalar/Hoffman,%202012.pdf), which seems to be one of the most-cited papers in the field of biomedical applications (if I'm interpreting these [Google Scholar results](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=biomedical+applications&btnG=) correctly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc21396",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "id": "eht6-fWVYARe",
    "outputId": "bcf87da9-fb71-45f0-c310-017e2ba39726"
   },
   "outputs": [],
   "source": [
    "text = (\n",
    "    \"Hydrogels are hydrophilic polymer networks which may absorb from \"\n",
    "    \"10–20% (an arbitrary lower limit) up to thousands of times their \"\n",
    "    \"dry weight in water.\"\n",
    ")\n",
    "\n",
    "# Split the sentence into tokens, with both BERT and SciBERT.\n",
    "bert_tokens = bert_tokenizer.tokenize(text)\n",
    "scibert_tokens = scibert_tokenizer.tokenize(text)\n",
    "\n",
    "# Pad out the scibert list to be the same length.\n",
    "while len(scibert_tokens) < len(bert_tokens):\n",
    "    scibert_tokens.append(\"\")\n",
    "\n",
    "# Label the columns.\n",
    "print(\"{:<12} {:<12}\".format(\"BERT\", \"SciBERT\"))\n",
    "print(\"{:<12} {:<12}\".format(\"----\", \"-------\"))\n",
    "\n",
    "# Display the tokens.\n",
    "for tup in zip(bert_tokens, scibert_tokens):\n",
    "    print(\"{:<12} {:<12}\".format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40117c76",
   "metadata": {
    "id": "Y9KJ779ylBW8"
   },
   "source": [
    "SciBERT apparently has embeddings for the words 'hydrogels' and 'hydrophillic', whereas BERT had to break these down into three subwords each. (Remember that the '##' in a token is just a way to flag it as a subword that is not the first subword). Apparently BERT does have \"polymer\", though!\n",
    "\n",
    "I skimmed the paper and pulled out some other esoteric terms--check out the different numbers of tokens required by each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d03b58",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "id": "OLfhdIHwdLnK",
    "outputId": "86699b0b-66d9-4af1-aa63-84844dde0bb8"
   },
   "outputs": [],
   "source": [
    "# Some strange terms from the paper.\n",
    "words = [\"polymerization\", \"2,2-azo-isobutyronitrile\", \"multifunctional crosslinkers\"]\n",
    "\n",
    "# For each term...\n",
    "for word in words:\n",
    "\n",
    "    # Print it out\n",
    "    print(\"\\n\\n\", word, \"\\n\")\n",
    "\n",
    "    # Start a list of tokens for each model, with the first one being the model name.\n",
    "    list_a = [\"BERT:\"]\n",
    "    list_b = [\"SciBERT:\"]\n",
    "\n",
    "    # Run both tokenizers.\n",
    "    list_a.extend(bert_tokenizer.tokenize(word))\n",
    "    list_b.extend(scibert_tokenizer.tokenize(word))\n",
    "\n",
    "    # Pad the lists to the same length.\n",
    "    while len(list_a) < len(list_b):\n",
    "        list_a.append(\"\")\n",
    "    while len(list_b) < len(list_a):\n",
    "        list_b.append(\"\")\n",
    "\n",
    "    # Wrap them in a DataFrame to display a pretty table.\n",
    "    df = pd.DataFrame([list_a, list_b])\n",
    "\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7de931c",
   "metadata": {
    "id": "kCl4ZLrzwkyq"
   },
   "source": [
    "The fact that SciBERT is able to represent all of these terms in fewer tokens seems like a good sign!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d07303",
   "metadata": {
    "id": "7UB5FRPOhinf"
   },
   "source": [
    "##### Vocab Dump\n",
    "\n",
    "It can be pretty interesting just to dump the full vocabulary of a model into a text file and skim it to see what stands out.\n",
    "\n",
    "This cell will write out SciBERT's vocab to 'vocabulary.txt', which you can open in Colab by going to the 'Files' tab in the pane on the left and double clicking the .txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88887b1",
   "metadata": {
    "id": "NRf7NXJGgsnG"
   },
   "outputs": [],
   "source": [
    "with open(\"vocabulary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "    # For each token in SciBERT's vocabulary...\n",
    "    for token in scibert_tokenizer.vocab.keys():\n",
    "\n",
    "        # Write it out, one per line.\n",
    "        f.write(token + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5938cec0",
   "metadata": {
    "id": "c8geTQ3Iidsv"
   },
   "source": [
    "You'll see that roughly the first 100 tokens are reserved, and then it looks like the rest of the vocabulary is sorted by frequency... The first actual tokens are:\n",
    "\n",
    "`t`, `a`, `##in`, `##he`, `##re`, `##on`, `the`, `s`, `##ti`\n",
    "\n",
    "> *Because the tokenizer breaks down \"unknown\" words into subtokens, it makes sense that some individual characters and subwords would be higher in frequency even than the most common words like \"the\".*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8f8ae8",
   "metadata": {
    "id": "zWU87CHqxgRt"
   },
   "source": [
    "##### Numbers and Symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c342a8",
   "metadata": {
    "id": "HFgt0WQ9Noch"
   },
   "source": [
    "There seem to be a lot of number-related tokens in SciBERT--you see them constantly as you scroll through the vocabulary. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502c881d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "pZY_Xd2iL3SS",
    "outputId": "be3a40e9-a6cb-4f78-864c-915f1588f472"
   },
   "outputs": [],
   "source": [
    "\"##.2%)\" in scibert_tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f71b74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "jaO-Eak5L_4P",
    "outputId": "3ef2dda5-751d-4a34-f678-47bd36e7202f"
   },
   "outputs": [],
   "source": [
    "\"0.36\" in scibert_tokenizer.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdc7dea",
   "metadata": {
    "id": "1aLXkM4MrAJ8"
   },
   "source": [
    "In the below loops, we'll tally up the number of tokens which include a digit, and show a random sample of these tokens. We'll do this for both SciBERT and BERT for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9567acbe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "id": "jUFGHG_Kpmw6",
    "outputId": "66026d92-1fe8-4105-abe3-bb9cb07b64ad"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# ======== BERT ========\n",
    "bert_examples = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "# For each token in the vocab...\n",
    "for token in bert_tokenizer.vocab:\n",
    "\n",
    "    # If there's a digit in the token...\n",
    "    # (But don't count those reserved tokens, e.g. \"[unused59]\")\n",
    "    if any(i.isdigit() for i in token) and not (\"unused\" in token):\n",
    "        # Count it.\n",
    "        count += 1\n",
    "\n",
    "        # Keep ~1% as examples to print.\n",
    "        if random.randint(0, 100) == 1:\n",
    "            bert_examples.append(token)\n",
    "\n",
    "# Calculate the count as a percentage of the total vocab.\n",
    "prcnt = float(count) / len(bert_tokenizer.vocab)\n",
    "\n",
    "# Print the result.\n",
    "print(\"In BERT:    {:>5,} tokens ({:.2%}) include a digit.\".format(count, prcnt))\n",
    "\n",
    "# ======== SciBERT ========\n",
    "scibert_examples = []\n",
    "count = 0\n",
    "\n",
    "# For each token in the vocab...\n",
    "for token in scibert_tokenizer.vocab:\n",
    "\n",
    "    # If there's a digit in the token...\n",
    "    # (But don't count those reserved tokens, e.g. \"[unused59]\")\n",
    "    if any(i.isdigit() for i in token) and not (\"unused\" in token):\n",
    "        # Count it.\n",
    "        count += 1\n",
    "\n",
    "        # Keep ~1% as examples to print.\n",
    "        if random.randint(0, 100) == 1:\n",
    "            scibert_examples.append(token)\n",
    "\n",
    "\n",
    "# Calculate the count as a percentage of the total vocab.\n",
    "prcnt = float(count) / len(scibert_tokenizer.vocab)\n",
    "\n",
    "# Print the result.\n",
    "print(\"In SciBERT: {:>5,} tokens ({:.2%}) include a digit.\".format(count, prcnt))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Examples from BERT:\", bert_examples)\n",
    "print(\"Examples from SciBERT:\", scibert_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf705c4e",
   "metadata": {
    "id": "-IY_eTkmscaK"
   },
   "source": [
    "So it looks like:\n",
    "- SciBERT has about 3x as many tokens with digits. \n",
    "- BERT's tokens are whole integers, and many look like they could be dates. (In [another Notebook](https://colab.research.google.com/drive/1fCKIBJ6fgWQ-f6UKs7wDTpNTL9N-Cq9X#scrollTo=-M1biDEVYjaL), I showed that BERT contains 384 of the integers in the range 1600 - 2021).\n",
    "- SciBERT's number tokens are much more diverse. They are often subwords, and many include decimal places or  other symbols like `%` or `(`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b02dd64",
   "metadata": {
    "id": "j4uNOCxvNwcG"
   },
   "source": [
    "Random -- check out token 17740!:\n",
    "\n",
    "⎝\n",
    "\n",
    "Looks like something is stuck to your monitor!  o_O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb4afb7",
   "metadata": {
    "id": "NjzQ6dKzMW3A"
   },
   "source": [
    "### Comparing Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b7fa90",
   "metadata": {
    "id": "d_A1kS9zHXp2"
   },
   "source": [
    "**Semantic Similarity on Scientific Text**\n",
    "\n",
    "To create a simple demonstration of the differences in the word and sentence level embeddings, we can invesitgate how each model creates embeddings for \"domain\" specific words and documents. It may be a stretch to call one embedding space better than another, but we predict a domain trained PLM will produce embeddings that cluster together more appropriately than a more general model. A simple approach is to calculate the cosine similarity between embeddings of domain words/documents that should or should not be close together based on human intuition.\n",
    "\n",
    "We thought our code and results are interesting to share all the same. \n",
    "\n",
    "For a paper that more vigurously compared a scientific-text domain trained BERT see this paper [here](https://arxiv.org/abs/1903.10676). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a8c1dd",
   "metadata": {
    "id": "7CSm_6ng1ufC"
   },
   "source": [
    "**Our Approach**\n",
    "\n",
    "In our semantic similarity task, we have three pieces of text--call them \"query\", \"A\", and \"B\", that are all on scientific topics. We pick these such that the query text is always more similar to A than to B. \n",
    "\n",
    "Here's an example:\n",
    "\n",
    "* query: \"Mitochondria (mitochondrion, singular) are membrane-bound cell organelles.\"\n",
    "* A: \"These powerhouses of the cell produce adenosine triphosphate (ATP).\"\n",
    "* B: \"Ribosomes contain RNA and are responsible for synthesizing the proteins needed for many cellular functions.\"\n",
    "\n",
    "`query` and `A` are both about mitochondria, whereas `B` is about ribosomes. However, to recognize the similarity between `query` and `A`, you would need to know that mitochondria are responsible for producing ATP.  \n",
    "\n",
    "Our intuition was that SciBERT, being trained on biomedical text, would better distinguish the similarities than BERT. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec805ad",
   "metadata": {
    "id": "CIBYucz_3yQQ"
   },
   "source": [
    "**Interpreting Cosine Similarities**\n",
    "\n",
    "When comparing two different models for semantic similarity, it's best to look at how well they *rank* the similarities, and not to compare the specific cosine similarity *values* across the two models.\n",
    "\n",
    "It's for this reason that we've structured our example as \"is `query` more similar to `A` or to `B`?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df399fb0",
   "metadata": {
    "id": "PJZL5KflODld"
   },
   "source": [
    "**Embedding Functions**\n",
    "\n",
    "In order to try out different examples, we've defined a `get_embedding` function below. It takes the average of the embeddings from the second-to-last layer or the final layer of the model to use as a sentence embedding.\n",
    "\n",
    "`get_embedding` also supports calculating an embedding for a specific word or sequence of words within the sentence. \n",
    "\n",
    "To locate the indeces of the tokens for these words, we've also defined the `get_word_indeces` helper function below. \n",
    "\n",
    "To calculate the word embedding, we again take the average of its token embeddings from the second-to-last layer of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efb10d4",
   "metadata": {
    "id": "NMLNJuNCiHE6"
   },
   "source": [
    "#### get_word_indeces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40f97a7",
   "metadata": {
    "id": "BMORSn8rmtJf"
   },
   "outputs": [],
   "source": [
    "def get_word_indeces(tokenizer, text, word):\n",
    "    \"\"\"\n",
    "    Determines the index or indeces of the tokens corresponding to `word`\n",
    "    within `text`. `word` can consist of multiple words, e.g., \"cell biology\".\n",
    "\n",
    "    Determining the indeces is tricky because words can be broken into multiple\n",
    "    tokens. I've solved this with a rather roundabout approach--I replace `word`\n",
    "    with the correct number of `[MASK]` tokens, and then find these in the\n",
    "    tokenized result.\n",
    "    \"\"\"\n",
    "    # Tokenize the 'word'--it may be broken into multiple tokens or subwords.\n",
    "    word_tokens = tokenizer.tokenize(word)\n",
    "    # Create a sequence of `[MASK]` tokens to put in place of `word`.\n",
    "    # get the tokenizers version of MASK - it differs between roberta and bert\n",
    "    mask_token = tokenizer.mask_token\n",
    "\n",
    "    masks_str = \" \".join([mask_token] * len(word_tokens))\n",
    "\n",
    "    # Replace the word with mask tokens.\n",
    "    text_masked = text.replace(word, masks_str)\n",
    "    # `encode` performs multiple functions:\n",
    "    #   1. Tokenizes the text\n",
    "    #   2. Maps the tokens to their IDs\n",
    "    #   3. Adds the special [CLS] and [SEP] tokens.\n",
    "    input_ids = tokenizer.encode(text_masked)\n",
    "    # print(f\"input ids are: {input_ids}\")\n",
    "    # print(f\"tokenizer mask id is:{tokenizer.mask_token_id}\")\n",
    "    # Use numpy's `where` function to find all indeces of the [MASK] token.\n",
    "    mask_token_indeces = np.where(np.array(input_ids) == tokenizer.mask_token_id)[0]\n",
    "\n",
    "    return mask_token_indeces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cefc5e",
   "metadata": {
    "id": "_vclJjNvwlmx"
   },
   "source": [
    "#### get_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416b338c",
   "metadata": {
    "id": "giJMGiGZhLLa"
   },
   "outputs": [],
   "source": [
    "def get_embedding(b_model, b_tokenizer, text, word=\"\", method=\"last_hidden\"):\n",
    "    \"\"\"\n",
    "    Uses the provided model and tokenizer to produce an embedding for the\n",
    "    provided `text`, and a \"contextualized\" embedding for `word`, if provided.\n",
    "\n",
    "    NOTE this only works on one sample at a time - not a batch\n",
    "    \"\"\"\n",
    "\n",
    "    # If a word is provided, figure out which tokens correspond to it.\n",
    "    if not word == \"\":\n",
    "        word_indeces = get_word_indeces(b_tokenizer, text, word)\n",
    "        # print(f\"Word indeces are:{word_indeces}\")\n",
    "\n",
    "    # Encode the text, adding the (required!) special tokens, and converting to\n",
    "    # PyTorch tensors.\n",
    "    encoded_dict = b_tokenizer.encode_plus(\n",
    "        text,  # Sentence to encode.\n",
    "        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "        return_tensors=\"pt\",  # Return pytorch tensors.\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_dict[\"input_ids\"]\n",
    "\n",
    "    b_model.eval()\n",
    "\n",
    "    # Run the text through the model and get the hidden states.\n",
    "    bert_outputs = b_model(input_ids)\n",
    "\n",
    "    # Run the text through BERT, and collect all of the hidden states produced\n",
    "    # from all 12 layers.\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = b_model(input_ids)\n",
    "\n",
    "        # Evaluating the model will return a different number of objects based on\n",
    "        # how it's  configured in the `from_pretrained` call earlier. In this case,\n",
    "        # becase we set `output_hidden_states = True`, the third item will be the\n",
    "        # hidden states from all layers. See the documentation for more details:\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#AutoModel\n",
    "\n",
    "        # print(f\"outputs are:{outputs}\")\n",
    "        if method == \"last_hidden\":\n",
    "            # the 0th element will be the last layer hidden states for each token (1,sequence_length, embed_dims)\n",
    "            # we just grab the 0th element of this, essentially equivalent to torch.squeeze to get (sequence_length, embed_dims)\n",
    "            token_vecs = outputs[0][0]\n",
    "            # print(f\"token_vecs shape: {token_vecs.shape}\")\n",
    "\n",
    "        elif method == \"second_last\":\n",
    "            # get all layers hidden states\n",
    "            hidden_states = outputs[2]\n",
    "            # `hidden_states` has shape [13 x 1 x <sentence length> x 768]\n",
    "\n",
    "            # Select the embeddings from the second to last layer.\n",
    "            # `token_vecs` is a tensor with shape [<sent length> x 768]\n",
    "            token_vecs = hidden_states[-2][0]\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    # Calculate the average of all token vectors.\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "\n",
    "    # Convert to numpy array.\n",
    "    sentence_embedding = sentence_embedding.detach().numpy()\n",
    "\n",
    "    # If `word` was provided, compute an embedding for those tokens.\n",
    "    if not word == \"\":\n",
    "        # Take the average of the embeddings for the tokens in `word`.\n",
    "        word_embedding = torch.mean(token_vecs[word_indeces], dim=0)\n",
    "\n",
    "        # Convert to numpy array.\n",
    "        word_embedding = word_embedding.detach().numpy()\n",
    "\n",
    "        return (sentence_embedding, word_embedding)\n",
    "    else:\n",
    "        return sentence_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ecb612",
   "metadata": {
    "id": "6FyjdEFRwUty"
   },
   "source": [
    "Retrieve the models and tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9458c057",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "omt9VRwYoJSB",
    "outputId": "8b7a0b87-c5a9-4306-b135-9a573526d08c"
   },
   "outputs": [],
   "source": [
    "# Retrieve SciBERT.\n",
    "scibert_model = AutoModel.from_pretrained(\n",
    "    \"allenai/scibert_scivocab_uncased\", output_hidden_states=True, cache_dir=cache_dir\n",
    ")\n",
    "scibert_tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "scibert_model.eval()\n",
    "\n",
    "# Retrieve generic BERT.\n",
    "bert_model = AutoModel.from_pretrained(\n",
    "    \"bert-base-uncased\", output_hidden_states=True, cache_dir=cache_dir\n",
    ")\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae54010d",
   "metadata": {
    "id": "GP_Uv1i8w9FV"
   },
   "source": [
    "Make a function to load and store multuiple models and tokenizers - \n",
    "\n",
    "___WARNING___ this is RAM heavy so use with caution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f12315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_tokenizers(model_names_or_paths: list = [], cache_dir: str = None):\n",
    "\n",
    "    \"\"\"\n",
    "    Function:\n",
    "            Read in and store models/tokenizers based on provided model_names_or_paths using Transformers AutoModel/AutoTokenizer classes.\n",
    "\n",
    "    Args:\n",
    "        model_names_or_paths: list -> comma separated list of model names found on HF hub or local paths\n",
    "        cache_\n",
    "    Returns:\n",
    "\n",
    "            A separate model and tokenizer dictionary containing entries for each model in the list by the same name. Can be accessed by model/tokenizer[model_name]\n",
    "    \"\"\"\n",
    "\n",
    "    # create an empty dict to store the different models/tokenizers\n",
    "    model_dict = {}\n",
    "    tokenizer_dict = {}\n",
    "\n",
    "    # load in each model and add to the dictionary for later use\n",
    "    for model_name in tqdm(model_names_or_paths):\n",
    "        # load model\n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_name, output_hidden_states=True, cache_dir=cache_dir\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "\n",
    "        # add to dict\n",
    "        # if a local path was provided we will want to extract a more useful name for the dictionary\n",
    "        # crude regex for now - based on how we created these models so very hard coded\n",
    "        if \"declutr_nhs\" in model_name:\n",
    "            model_name = \"declutr_nhs_roberta\"\n",
    "        elif \"roberta-base-NHS-incidents\" in model_name:\n",
    "            model_name = \"roberta_NHS_incidents\"\n",
    "        else:\n",
    "            raise NotImplementedError  # Add any regex rules you want here to rename model_name after loading the model/tokenizer\n",
    "        # add the model_name to the dictionaries\n",
    "        model_dict[model_name] = model\n",
    "        tokenizer_dict[model_name] = tokenizer\n",
    "\n",
    "    return model_dict, tokenizer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3428ed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in multiple models for later use - this will take up a lot of RAM if many models\n",
    "model_names_or_paths = [\n",
    "    \"roberta-base\",\n",
    "    \"allenai/scibert_scivocab_uncased\",\n",
    "    \"bert-base-uncased\",\n",
    "    \"johngiorgi/declutr-sci-base\",\n",
    "    \"johngiorgi/declutr-base\",\n",
    "]\n",
    "\n",
    "cache_dir = \"CACHE_DIR\"\n",
    "\n",
    "all_models, all_tokenizers = get_model_tokenizers(model_names_or_paths, cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b6d0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the model keys i.e. the model_names\n",
    "all_models.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9527c983",
   "metadata": {},
   "source": [
    "#### quick example with one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dba707",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "PGoJ15Mmik_F",
    "outputId": "96d32361-dbc6-4a7c-ad09-df405349f503"
   },
   "outputs": [],
   "source": [
    "model_name = \"roberta-base\"\n",
    "\n",
    "text = \"hydrogels are hydrophilic polymer networks which may absorb from 10–20% (an arbitrary lower limit) up to thousands of times their dry weight in water.\"\n",
    "word = \"hydrogels\"\n",
    "\n",
    "# Get the embedding for the sentence, as well as an embedding for 'hydrogels'.\n",
    "(sen_emb, word_emb) = get_embedding(\n",
    "    all_models[model_name], all_tokenizers[model_name], text, word\n",
    ")\n",
    "\n",
    "print(\"Embedding sizes:\")\n",
    "print(sen_emb.shape)\n",
    "print(word_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0558aa6e",
   "metadata": {
    "id": "6MensSbiw4nn"
   },
   "source": [
    "Here's the code for calculating cosine similarity. We'll test it by comparing the word embedding with the sentence embedding--not a very interesting comparison, but a good sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5424d118",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "nPNIJ8vRw4-K",
    "outputId": "4d9e297c-efa8-43bd-b77f-932e09fac547"
   },
   "outputs": [],
   "source": [
    "# Calculate the cosine similarity of the two embeddings.\n",
    "sim = 1 - cosine(sen_emb, word_emb)\n",
    "\n",
    "print(\"Cosine similarity: {:.2}\".format(sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c51e205",
   "metadata": {},
   "source": [
    "#### Alternative method for calculating embeddings\n",
    "\n",
    "This method is slightly different in that it takes the last layers hidden states and uses the attention mask to calculate batch wide means and will be easier for processing batches of inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f913a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"roberta-base\"\n",
    "\n",
    "text = [\n",
    "    \"hydrogels are hydrophilic polymer networks which may absorb from 10–20% (an arbitrary lower limit) up to thousands of times their dry weight in water.\",\n",
    "    \"hydrogels smell\",\n",
    "    \"cheese is really nice\",\n",
    "]\n",
    "\n",
    "inputs = all_tokenizers[model_name](\n",
    "    text, padding=True, truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# embed text\n",
    "with torch.no_grad():\n",
    "    # get last hidden states for each token\n",
    "    sequence_output = all_models[model_name](**inputs)[0]\n",
    "\n",
    "# mean pool the token-level embeddings to get sentence-level\n",
    "embeddings = torch.sum(\n",
    "    sequence_output * inputs[\"attention_mask\"].unsqueeze(-1), dim=1\n",
    ") / torch.clamp(torch.sum(inputs[\"attention_mask\"], dim=1, keepdims=True), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa5596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc76060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get similarity via cosine distance for the first and second sentence\n",
    "semantic_sim = 1 - cosine(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7981a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e6f416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check that calculating the pairwise embeddings for all possible pairs\n",
    "# the [0][1] and [1][0] element of the matrix should be the semantic sim in the above cell\n",
    "cosine_similarity(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a2f5a",
   "metadata": {
    "id": "mYGiIc6DPEJL"
   },
   "source": [
    "### Sentence Comparison Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7b951c",
   "metadata": {},
   "source": [
    "#### Compare an anchor to multiple queries\n",
    "\n",
    "Here we will provide a set of sentneces as a list or batch, and the first or the 0th element of that list will be used as an *anchor* and the rest treated as *queries*. The embeddings will be calculated for each and the cosine similarity between the *anchor* and all other *queries* will be calculated. The entire pairwise similarity matrix will also be returned for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aabab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to take in a batch of sentences, compute embeddings and calculate similarity and/or visualise embedddings\n",
    "def batch_compare_sentence_embs(\n",
    "    texts: list = [],\n",
    "    model_names: list = None,\n",
    "    cache_dir: str = None,\n",
    "    device: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Function:\n",
    "            Will take in a number models and produce embeddings for provided texts.\n",
    "            Cosine distance/similarity will be calculated between document embeddings for each model and\n",
    "            optionally the embeddings will be passed through UMAP to allow plotting in 2D.\n",
    "    \"\"\"\n",
    "\n",
    "    # if a single string is provided for model names put into a list\n",
    "    if type(model_names) == str:\n",
    "        model_names = [model_names]\n",
    "\n",
    "    # separate the anchor and query texts\n",
    "    anchor_text = texts[0]\n",
    "    query_texts = texts[1:]\n",
    "\n",
    "    # create initial dataframe to store the texts\n",
    "    results_df = pd.DataFrame({\"anchor_text\": anchor_text, \"query_text\": query_texts})\n",
    "\n",
    "    # create dict to store model specific full cosine similarity matrix\n",
    "    cosine_sim_matrices = {}\n",
    "\n",
    "    print(f\"Anchor text:\\n\\n{texts[0]}\")\n",
    "    print(f\"=\" * 50)\n",
    "\n",
    "    # cycle through models provided:\n",
    "\n",
    "    for model_name in tqdm(model_names, desc=\"Processing\"):\n",
    "\n",
    "        model = AutoModel.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "\n",
    "        # if a path was provided we will want to extract a more useful name for the dictionary\n",
    "        # crude regex for now - based on how we created these models so very hard coded\n",
    "        if \"declutr_nhs\" in model_name:\n",
    "            model_name = \"declutr_roberta_nhs_incident\"\n",
    "        elif \"roberta-base-NHS-incidents\" in model_name:\n",
    "            model_name = \"roberta_NHS_incidents\"\n",
    "        elif \"incident/declutr-base/\" in model_name:\n",
    "            model_name = \"declutr_base_nhs_incident\"\n",
    "        else:\n",
    "            raise NotImplementedError  # add any regex rules to rename model_names here if desired\n",
    "\n",
    "        inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # embed text\n",
    "        with torch.no_grad():\n",
    "            # get last hidden states for each token\n",
    "            sequence_output = model(**inputs)[0]\n",
    "\n",
    "        # mean pool the token-level embeddings to get sentence-level\n",
    "        embeddings = torch.sum(\n",
    "            sequence_output * inputs[\"attention_mask\"].unsqueeze(-1), dim=1\n",
    "        ) / torch.clamp(\n",
    "            torch.sum(inputs[\"attention_mask\"], dim=1, keepdims=True), min=1e-9\n",
    "        )\n",
    "\n",
    "        # print(f\"Embeddings calculated of shape: {embeddings.shape}\")\n",
    "\n",
    "        # presume that the anchor text is the 0th element of the provided texts list\n",
    "        anchor_emb = embeddings[0]\n",
    "\n",
    "        # assign the non-anchor embeddings to another variable\n",
    "        query_embs = embeddings[1:]\n",
    "\n",
    "        # now get similarity via cosine distance\n",
    "        semantic_sim = 1 - cosine(embeddings[0], embeddings[1])\n",
    "        cosine_sims = [\n",
    "            (1 - cosine(query_embs[i], anchor_emb)) for i in range(query_embs.size(0))\n",
    "        ]\n",
    "\n",
    "        # add similarities to a table or something\n",
    "        results_df[f\"{model_name}_cs\"] = cosine_sims\n",
    "\n",
    "        # add full cosine similarity matrix to the dictionary\n",
    "        cosine_sim_matrices[model_name] = cosine_similarity(embeddings)\n",
    "\n",
    "    return results_df, cosine_sim_matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea27598",
   "metadata": {},
   "source": [
    "function to find the **most** similary based on cosine distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ecc20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional function to find the\n",
    "def most_similar(texts, doc_id, similarity_matrix, matrix):\n",
    "    print(f\"Document: {texts[doc_id]}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"Similar Documents using {matrix}:\")\n",
    "    if matrix == \"Cosine Similarity\":\n",
    "        similar_ix = np.argsort(similarity_matrix[doc_id])[::-1]\n",
    "    elif matrix == \"Euclidean Distance\":\n",
    "        similar_ix = np.argsort(similarity_matrix[doc_id])\n",
    "    for ix in similar_ix:\n",
    "        if ix == doc_id:\n",
    "            continue\n",
    "        print(\"\\n\")\n",
    "        print(f\"Document: {texts[ix]}\")\n",
    "        print(f\"{matrix} : {similarity_matrix[doc_id][ix]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4922dd43",
   "metadata": {},
   "source": [
    "### Compare a incident report specific anchor with both similar and dissimilar domain sentences and also general domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cce1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO  - have a incident specific anchor with:\n",
    "\n",
    "texts = [\n",
    "    \"The patient fell off the bed and was left unattended for 3 hours.\",\n",
    "    \"The man was running for cover as it was raining and fell.\",\n",
    "    \"Tension grew between the two nations as stocks ran low.\",\n",
    "    \"Bedside incident. Pt left prone for 4 hours. Breach.\",\n",
    "    \"RTT for patient with high blood pressure gone unchecked.\",\n",
    "    \"The patient fell off the bed and was laying unattended for 3 hours\",\n",
    "    \"Left unattended after patient fell off bed.\",\n",
    "    \"The movie was amazing with some great acting performances.\",\n",
    "    \"Awful film with subpar performances from all actors involved.\",\n",
    "]\n",
    "\n",
    "# supply list of models of interest\n",
    "model_names_or_paths = [\"roberta-base\", \"bert-base-uncased\", \"johngiorgi/declutr-base\"]\n",
    "\n",
    "embed_results, cosine_sim_matrices = batch_compare_sentence_embs(\n",
    "    texts=texts, model_names=model_names_or_paths, cache_dir=cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1163759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow pandas dataframe to display whole columns for easier reading\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(embed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c2ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_results.iloc[7, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3130d986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieval the full cosine similarity matrix for one of the models using the model name\n",
    "cosine_sim_matrices[\"roberta-base\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca259b",
   "metadata": {},
   "source": [
    "plot the cosine similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f728ba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "in_arr = cosine_sim_matrices[\"roberta-base\"]\n",
    "\n",
    "ax.matshow(in_arr)\n",
    "\n",
    "for i in range(in_arr.shape[0]):\n",
    "    for j in range(in_arr.shape[1]):\n",
    "        c = in_arr[j, i]\n",
    "        ax.text(i, j, str(c)[:4], va=\"center\", ha=\"center\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c391c1b2",
   "metadata": {},
   "source": [
    "Can make a graph plot using network x using the cosine simialrity as a threshold for whether a link is made. Very simply and can be explored further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293005fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_numpy_array(cosine_sim_matrices[\"roberta-base\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da16edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(G.edges(data=\"weight\"), key=lambda x: x[2])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4aaf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND = 0.6\n",
    "\n",
    "for n, text in enumerate(texts):\n",
    "    print(f\"{n}: {text}\")\n",
    "\n",
    "print(f\"\\nLower bound cutoff for connections: {LOWER_BOUND}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "pos = nx.spring_layout(G, seed=0)\n",
    "nx.draw(G, pos, with_labels=True, width=0)\n",
    "\n",
    "edge_minimum = sorted(G.edges(data=\"weight\"), key=lambda x: x[2])[0][2]\n",
    "\n",
    "for edge in G.edges(data=\"weight\"):\n",
    "    if edge[2] > LOWER_BOUND:\n",
    "        nx.draw_networkx_edges(G, pos, edgelist=[edge], width=2)\n",
    "    else:\n",
    "        nx.draw_networkx_edges(G, pos, edgelist=[edge], width=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09115d5",
   "metadata": {},
   "source": [
    "Can also make a more interactive version of the above using [d3blocks](https://github.com/d3blocks/d3blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d926f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "d3 = D3Blocks()\n",
    "#\n",
    "# Import example\n",
    "df = pd.DataFrame(nx.to_pandas_edgelist(G))\n",
    "df[\"weight\"] = df[\"weight\"].apply(lambda x: x * 100)\n",
    "\n",
    "# Create network using default\n",
    "d3.d3graph(df, filepath=\"./d3graph.html\", slider=[0, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebb7eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(texts, 0, cosine_sim_matrices[\"roberta-base\"], \"Cosine Similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed6f1fc",
   "metadata": {},
   "source": [
    "### Try some interesting examples for each model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8939eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"The patient fell off the bed and was left unattended for 3 hours.\",\n",
    "    \"Left unattended after patient fell off bed.\",\n",
    "]\n",
    "\n",
    "# load in multiple models for later use - this will take up a lot of RAM if many models\n",
    "model_names_or_paths = [\"roberta-base\", \"<LOCAL_MODEL_PATH>\", \"johngiorgi/declutr-base\"]\n",
    "\n",
    "embed_results, cosine_sim_matrices = batch_compare_sentence_embs(\n",
    "    texts=texts, model_names=model_names_or_paths, cache_dir=\"<CACHE_DIR>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bea4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(embed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a9aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"The patient fell off the bed.\",  # and was left unattended for 3 hours.\",\n",
    "    \"The movie was amazing.\",  # with some great acting performances.\"\n",
    "]\n",
    "\n",
    "# load in multiple models for later use - this will take up a lot of RAM if many models\n",
    "model_names_or_paths = [\"roberta-base\", \"<LOCAL_MODEL_PATH>\", \"johngiorgi/declutr-base\"]\n",
    "\n",
    "embed_results, cosine_sim_matrices = batch_compare_sentence_embs(\n",
    "    texts=texts, model_names=model_names_or_paths, cache_dir=\"<CACHE_DIR>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4307bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(embed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a7c4e8",
   "metadata": {
    "id": "WOmJjM5KyxsZ"
   },
   "source": [
    "#### Word Comparison Examples\n",
    "We can also look at the embeddings of single words in different contexts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5947b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_models.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e232065",
   "metadata": {},
   "source": [
    "This is a bit of a messy notebook at this point, but in its current form certain functions work better on a one sample at a time basis, others work better with batches. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac7b3a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "id": "BpUdEK5k8xx_",
    "outputId": "d29235f5-bd16-42b1-bf7d-45ecb5b78ea3"
   },
   "outputs": [],
   "source": [
    "# this loop is reliant upon having loaded and stored models and tokenizers in dictionary objects as per an earlier cell\n",
    "for model_name in tqdm(all_models.keys()):\n",
    "    text = \"Pt was acting aggressive towards another patient.\"\n",
    "\n",
    "    print('\"' + text + '\"\\n')\n",
    "\n",
    "    # Get contextualized embeddings for \"prison\", \"animal\", and \"cell\"\n",
    "    (emb_sen, emb_pt) = get_embedding(\n",
    "        all_models[model_name], all_tokenizers[model_name], text, word=\"pt\"\n",
    "    )\n",
    "    (emb_sen, emb_patient) = get_embedding(\n",
    "        all_models[model_name], all_tokenizers[model_name], text, word=\"patient\"\n",
    "    )\n",
    "    (emb_sen, emb_aggressive) = get_embedding(\n",
    "        all_models[model_name], all_tokenizers[model_name], text, word=\"aggressive\"\n",
    "    )\n",
    "\n",
    "    print(f\"{model_name}:\")\n",
    "\n",
    "    # Compare the embeddings\n",
    "\n",
    "    print(\"  sim(pt, patient): {:}\".format((1 - cosine(emb_pt, emb_patient))))\n",
    "    print(\n",
    "        \"  sim(patient, aggressive): {:}\".format(\n",
    "            1 - cosine(emb_patient, emb_aggressive)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f90479d",
   "metadata": {
    "id": "FYq0v6O3-tVG"
   },
   "source": [
    "Let us know if you find some more interesting examples to try!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bff0b03",
   "metadata": {
    "id": "-B36npqKZkph"
   },
   "source": [
    "# Appendix: BioBERT vs. SciBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c23caa0",
   "metadata": {
    "id": "J51vlJa9Zm_c"
   },
   "source": [
    "I don't have much insight into the merits of BioBERT versus SciBERT, but I thought I would at least share what I do know.\n",
    "\n",
    "**Publish Dates & Authors**\n",
    "\n",
    "* *BioBERT*\n",
    "    * First submitted to arXiv: `Jan 25th, 2019`\n",
    "        * [link](https://arxiv.org/abs/1901.08746)\n",
    "    * First Author: Jinhyuk Lee\n",
    "    * Organization: Korea University, Clova AI (also Korean)\n",
    "\n",
    "* *SciBERT*\n",
    "   * First submitted to arXiv: `Mar 26, 2019`\n",
    "       * [arXiv](https://arxiv.org/abs/1903.10676), [pdf](https://arxiv.org/pdf/1903.10676.pdf)\n",
    "    * First Author: Iz Beltagy\n",
    "    * Organization: Allen AI\n",
    "\n",
    "**Differences**\n",
    "\n",
    "* BioBERT used the same tokens as the original BERT, rather than choosing a new vocabulary of tokens based on their corpus. Their justification was \"to maintain compatibility\", which I don't entirely understand.\n",
    "* SciBERT learned a new vocabulary of tokens, but they also found that this detail is less important--it's training on the specialized corpus that really makes the difference.\n",
    "* SciBERT is more recent, and outperforms BioBERT on many, but not all, scientific NLP benchmarks.\n",
    "* The difference in naming seems unfortunate--SciBERT is also trained primarily on biomedical research papers, but the name \"BioBERT\" was already taken, so....\n",
    "\n",
    "**huggingface transformers**\n",
    "\n",
    "* Allen AI published their SciBERT models for the transformers library, and you can see their popularity:\n",
    "    * [SciBERT uncased](https://huggingface.co/allenai/scibert_scivocab_uncased): ~16.7K downloads (from 5/22/20 - 6/22/20)\n",
    "        * `allenai/scibert_scivocab_uncased`\n",
    "    * [SciBERT cased](https://huggingface.co/allenai/scibert_scivocab_cased ): ~3.8k downloads (from 5/22/20 - 6/22/20)\n",
    "        * `allenai/scibert_scivocab_cased`\n",
    "* The BioBERT team has published their models, but not for the `transformers` library, as far as I can tell. \n",
    "    * The most popular BioBERT model in the huggingface community appears to be [this one](https://huggingface.co/monologg/biobert_v1.1_pubmed): `monologg/biobert_v1.1_pubmed`, with ~8.6K downloads (from 5/22/20 - 6/22/20)\n",
    "       * You could also download BioBERT's pre-trained weights yourself from https://github.com/naver/biobert-pretrained, but I'm not sure what it would take to pull these into the `transformers` library exactly. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "elm4psir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:53:40) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "036f9b356400688fa32ab139d64151f7af42c87240ca002d464048bf8c685a85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
