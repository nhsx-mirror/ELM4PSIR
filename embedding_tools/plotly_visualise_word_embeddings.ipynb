{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fdc2526",
   "metadata": {},
   "source": [
    "### Visualising word embeddings for different PLMs\n",
    "\n",
    "This notebook is intended to create 2D or 3D visualisation of embeddings from transformer based PLMs like roberta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ab8088",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "\n",
    "# RobertaModel, RobertaTokenizer\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# visualization libs\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.spatial.distance import euclidean, pdist, squareform\n",
    "from sklearn import manifold  # use this for MDS computation\n",
    "from sklearn.metrics.pairwise import cosine_distances, euclidean_distances\n",
    "from transformers import AutoModel, AutoTokenizer, BertModel, BertTokenizer\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "# Used to calculation of word movers distance between sentence\n",
    "from collections import Counter\n",
    "\n",
    "# Library to calculate Relaxed-Word Movers distance\n",
    "from wmd import WMD, libwmdrelax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30178a24",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22784b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(data, tokenizer_obj, max_length):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    @return   attention_masks_without_special_tok (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model excluding the special tokens (CLS/SEP)\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer_obj.encode_plus(\n",
    "            text=sent,  # Preprocess sentence\n",
    "            add_special_tokens=True,  # Add `[CLS]` and `[SEP]`\n",
    "            max_length=max_length,  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,  # Pad sentence to max length\n",
    "            truncation=True,  # Truncate longer seq to max_len\n",
    "            return_attention_mask=True,  # Return attention mask\n",
    "        )\n",
    "\n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get(\"input_ids\"))\n",
    "        attention_masks.append(encoded_sent.get(\"attention_mask\"))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    # lets create another mask that will be useful when we want to average all word vectors later\n",
    "    # we would like to average across all word vectors in a sentence, but excluding the CLS and SEP token\n",
    "    # create a copy\n",
    "    attention_masks_without_special_tok = attention_masks.clone().detach()\n",
    "\n",
    "    # set the CLS token index to 0 for all sentences\n",
    "    attention_masks_without_special_tok[:, 0] = 0\n",
    "\n",
    "    # get sentence lengths and use that to set those indices to 0 for each length\n",
    "    # essentially, the last index for each sentence, which is the SEP token\n",
    "    sent_len = attention_masks_without_special_tok.sum(1).tolist()\n",
    "\n",
    "    # column indices to set to zero\n",
    "    col_idx = torch.LongTensor(sent_len)\n",
    "    # row indices for all rows\n",
    "    row_idx = torch.arange(attention_masks.size(0)).long()\n",
    "\n",
    "    # set the SEP indices for each sentence token to zero\n",
    "    attention_masks_without_special_tok[row_idx, col_idx] = 0\n",
    "\n",
    "    return input_ids, attention_masks, attention_masks_without_special_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda74e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will retrieve vector representations or embeddings at the word level\n",
    "def get_vector(hidden_layers_form_arch, token_index=0, mode=\"average\", top_n_layers=4):\n",
    "    \"\"\"\n",
    "    retrieve vectors for a token_index from the top n layers and return a concatenated, averaged or summed vector\n",
    "    hidden_layers_form_arch: tuple returned by the transformer library\n",
    "    token_index: index of the token for which a vector is desired\n",
    "    mode=\n",
    "          'average' : avg last n layers\n",
    "          'concat': concatenate last n layers\n",
    "          'sum' : sum last n layers\n",
    "          'last': return embeddings only from last layer\n",
    "          'second_last': return embeddings only from second last layer\n",
    "\n",
    "    top_n_layers: number of top layers to concatenate/ average / sum\n",
    "    \"\"\"\n",
    "    if mode == \"concat\":\n",
    "        # concatenate last 4 layer outputs -> returns [batch_size x seq_len x dim]\n",
    "        # permute(1,0,2) swaps the the batch and seq_len dim , making it easy to return all the vectors for a particular token position\n",
    "        return torch.cat(hidden_layers_form_arch[-top_n_layers:], dim=2).permute(\n",
    "            1, 0, 2\n",
    "        )[token_index]\n",
    "\n",
    "    if mode == \"average\":\n",
    "        # avg last 4 layer outputs -> returns [batch_size x seq_len x dim]\n",
    "        return (\n",
    "            torch.stack(hidden_layers_form_arch[-top_n_layers:])\n",
    "            .mean(0)\n",
    "            .permute(1, 0, 2)[token_index]\n",
    "        )\n",
    "\n",
    "    if mode == \"sum\":\n",
    "        # sum last 4 layer outputs -> returns [batch_size x seq_len x dim]\n",
    "        return (\n",
    "            torch.stack(hidden_layers_form_arch[-top_n_layers:])\n",
    "            .sum(0)\n",
    "            .permute(1, 0, 2)[token_index]\n",
    "        )\n",
    "\n",
    "    if mode == \"last\":\n",
    "        # last layer output -> returns [batch_size x seq_len x dim]\n",
    "        return hidden_layers_form_arch[-1:][0].permute(1, 0, 2)[token_index]\n",
    "\n",
    "    if mode == \"second_last\":\n",
    "        # last layer output -> returns [batch_size x seq_len x dim]\n",
    "        return hidden_layers_form_arch[-2:-1][0].permute(1, 0, 2)[token_index]\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b6aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_dists(\n",
    "    dists, labels, dims=2, reducer=None, words_of_interest=[], title=\"\", save_dir=\"./\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot distances using MDS in 2D/3D\n",
    "    dists: precomputed distance matrix\n",
    "    labels: labels to display on the plot\n",
    "    dims: 2/3 for 2 or 3 dimensional plot, defaults to 2 for any other value passed\n",
    "    words_of_interest: list of words to highlight with a different color\n",
    "    title: title for the plot\n",
    "    \"\"\"\n",
    "    cnt_dict = dict()\n",
    "    color = list()\n",
    "\n",
    "    # separate colors for words that are in words_of_interest vs other\n",
    "    # each word will have a _SentenceNumber at the end to differentiate the words coming in from different sentences\n",
    "    for v in labels:\n",
    "        found = False\n",
    "        for wrd_int in words_of_interest:\n",
    "            if wrd_int in v:\n",
    "                found = True\n",
    "                break\n",
    "\n",
    "        if found:\n",
    "            color.append(1)\n",
    "        else:\n",
    "            color.append(0)\n",
    "\n",
    "    # https://community.plotly.com/t/plotly-colours-list/11730/6\n",
    "    colorscale = [[0, \"darkcyan\"], [1, \"white\"]]\n",
    "\n",
    "    # dists is precomputed using cosine similarity and passed\n",
    "    # calculate MDS with number of dims passed\n",
    "\n",
    "    if reducer == \"MDS\":\n",
    "        mds = manifold.MDS(\n",
    "            n_components=dims,\n",
    "            dissimilarity=\"precomputed\",\n",
    "            random_state=60,\n",
    "            max_iter=90000,\n",
    "        )\n",
    "        results = mds.fit(dists)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # get coodinates for each point\n",
    "    coords = results.embedding_\n",
    "\n",
    "    # plot\n",
    "    if dims == 3:\n",
    "        fig = go.Figure(\n",
    "            data=[\n",
    "                go.Scatter3d(\n",
    "                    x=coords[:, 0],\n",
    "                    y=coords[:, 1],\n",
    "                    z=coords[:, 2],\n",
    "                    mode=\"markers+text\",\n",
    "                    textposition=\"top center\",\n",
    "                    text=labels,\n",
    "                    marker=dict(\n",
    "                        size=10,\n",
    "                        color=color,\n",
    "                        colorscale=colorscale,\n",
    "                        opacity=0.8,\n",
    "                    ),\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        fig = go.Figure(\n",
    "            data=[\n",
    "                go.Scatter(\n",
    "                    x=coords[:, 0],\n",
    "                    y=coords[:, 1],\n",
    "                    mode=\"markers+text\",\n",
    "                    text=labels,\n",
    "                    textposition=\"top center\",\n",
    "                    marker=dict(\n",
    "                        size=12,\n",
    "                        color=color,\n",
    "                        colorscale=colorscale,\n",
    "                        opacity=0.8,\n",
    "                    ),\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    fig.update_layout(template=\"plotly_dark\")\n",
    "    if title != \"\":\n",
    "        fig.update_layout(title_text=title)\n",
    "    # save to html for later use?\n",
    "    if save_dir != None:\n",
    "        fig.write_html(f\"{save_dir}\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9153f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_dists_subplots(\n",
    "    dists, labels, dims=2, reducer=None, words_of_interest=[], title=\"\", save_dir=\"./\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot distances using MDS in 2D/3D\n",
    "    dists: precomputed distance matrix\n",
    "    labels: labels to display on the plot\n",
    "    dims: 2/3 for 2 or 3 dimensional plot, defaults to 2 for any other value passed\n",
    "    words_of_interest: list of words to highlight with a different color\n",
    "    static_vectors: Boolean -> whether or not static vectors have been provided - if so, make each sample point a different colour\n",
    "    title: title for the plot\n",
    "    save_dir: path to save any created plots\n",
    "    \"\"\"\n",
    "    cnt_dict = dict()\n",
    "    color = list()\n",
    "\n",
    "    # separate colors for words that are in words_of_interest vs other\n",
    "    # each word will have a _SentenceNumber at the end to differentiate the words coming in from different sentences\n",
    "    for v in labels:\n",
    "        found = False\n",
    "        for wrd_int in words_of_interest:\n",
    "            if wrd_int in v:\n",
    "                found = True\n",
    "                break\n",
    "\n",
    "        if found:\n",
    "            color.append(1)\n",
    "        else:\n",
    "            color.append(0)\n",
    "\n",
    "    # https://community.plotly.com/t/plotly-colours-list/11730/6\n",
    "    colorscale = [[0, \"darkcyan\"], [1, \"white\"]]\n",
    "\n",
    "    # dists is precomputed using cosine similarity and passed\n",
    "    # calculate MDS with number of dims passed\n",
    "\n",
    "    if reducer == \"MDS\":\n",
    "        mds = manifold.MDS(\n",
    "            n_components=dims,\n",
    "            dissimilarity=\"precomputed\",\n",
    "            random_state=60,\n",
    "            max_iter=90000,\n",
    "        )\n",
    "        results = mds.fit(dists)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # get coodinates for each point\n",
    "    coords = results.embedding_\n",
    "\n",
    "    # plot\n",
    "    if dims == 3:\n",
    "        fig = go.Figure(\n",
    "            data=[\n",
    "                go.Scatter3d(\n",
    "                    x=coords[:, 0],\n",
    "                    y=coords[:, 1],\n",
    "                    z=coords[:, 2],\n",
    "                    mode=\"markers+text\",\n",
    "                    textposition=\"top center\",\n",
    "                    text=labels,\n",
    "                    marker=dict(\n",
    "                        size=10,\n",
    "                        color=color,\n",
    "                        colorscale=colorscale,\n",
    "                        opacity=0.8,\n",
    "                    ),\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        fig = go.Figure(\n",
    "            data=[\n",
    "                go.Scatter(\n",
    "                    x=coords[:, 0],\n",
    "                    y=coords[:, 1],\n",
    "                    mode=\"markers+text\",\n",
    "                    text=labels,\n",
    "                    textposition=\"top center\",\n",
    "                    marker=dict(\n",
    "                        size=12,\n",
    "                        color=color,\n",
    "                        colorscale=colorscale,\n",
    "                        opacity=0.8,\n",
    "                    ),\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    fig.update_layout(template=\"plotly_dark\")\n",
    "    if title != \"\":\n",
    "        fig.update_layout(title_text=title)\n",
    "    # save to html for later use?\n",
    "    if save_dir != None:\n",
    "        fig.write_html(f\"{save_dir}\")\n",
    "    # fig.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7642cd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_context_vecs(\n",
    "    input_hidden_states,\n",
    "    input_tokenized_sents,\n",
    "    mode=\"concat\",\n",
    "    model_name=\"bert-base-uncased\",\n",
    "    reducer=\"MDS\",\n",
    "    top_n_layers=4,\n",
    "    dims=2,\n",
    "    max_length=15,\n",
    "    sent_lengths=None,\n",
    "    words_with_diff_color=None,\n",
    "    save_dir=\"./\",\n",
    "):\n",
    "    \"\"\"\n",
    "    function to get a vectors for each word in each sentence, add the sentence number to the end of each word\n",
    "    calculate cosine distance between each pair of words and then pass it to the visualization function\n",
    "\n",
    "    inputs:\n",
    "    input_hidden_states: hiddent states retrieved from a BERT-like model\n",
    "    input_tokenized_sents: tokenized sentences, used to assign labels for each point on the plot\n",
    "    model:  'average' : avg last n layers\n",
    "            'concat': concatenate last n layers\n",
    "            'sum' : sum last n layers\n",
    "            'last':  embeddings only from last layer\n",
    "            'second_last':  embeddings only from second last layer\n",
    "    top_n_layers: top n layers to use for concat/sum etc.\n",
    "    viz_dims: 2/3 for 2D or 3D plot\n",
    "    words_with_diff_color: words that should be highlighed with different color on the plot\n",
    "    \"\"\"\n",
    "    vecs = list()\n",
    "    labels = list()\n",
    "    for token_ind in range(max_length):\n",
    "        if token_ind == 0:\n",
    "            # ignore CLS\n",
    "            continue\n",
    "        vectors = get_vector(\n",
    "            input_hidden_states,\n",
    "            token_index=token_ind,\n",
    "            mode=mode,\n",
    "            top_n_layers=top_n_layers,\n",
    "        )\n",
    "        for sent_ind, sent_len in enumerate(sent_lengths):\n",
    "            if token_ind < sent_len - 1:\n",
    "                # ignore SEP which will be at the last index of each sentence\n",
    "                vecs.append(vectors[sent_ind])\n",
    "                labels.append(\n",
    "                    input_tokenized_sents[sent_ind][token_ind] + \"_\" + str(sent_ind)\n",
    "                )\n",
    "\n",
    "    # create a numpy matrix to pass to cosine distance\n",
    "    mat = torch.stack(vecs).detach().numpy()\n",
    "    # call the plot function on the cosine distance matrix\n",
    "\n",
    "    plots = plt_dists_subplots(\n",
    "        cosine_distances(mat),\n",
    "        reducer=reducer,\n",
    "        labels=labels,\n",
    "        dims=dims,\n",
    "        words_of_interest=words_with_diff_color,\n",
    "        title=f\"Model: {model_name} and Method: {mode}\",\n",
    "        save_dir=f\"{save_dir}/{model_name}_{mode}_plotly_{dims}D.html\",\n",
    "    )\n",
    "    return plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459a735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_visualise_pipeline(\n",
    "    texts,\n",
    "    focus_word,\n",
    "    model_names,\n",
    "    cache_dir,\n",
    "    max_length,\n",
    "    reducer=None,\n",
    "    dims=2,\n",
    "    mode=\"concat\",\n",
    "    save_dir=\"./\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to run through the steps of loading model, tokenizing, extracting embedding, reducing dimensions and plotting for given text\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # we can run over mulitple model names to get multiple plots in one run\n",
    "\n",
    "    if len(model_names) == 1:\n",
    "\n",
    "        model_names = model_names[0]\n",
    "        # Initialize tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_names, cache_dir=cache_dir\n",
    "        )  # RobertaTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "\n",
    "        # initialize model\n",
    "        # output_hidden_states = True will give us all hiddent states for all layers\n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_names, output_hidden_states=True, cache_dir=cache_dir\n",
    "        )  # RobertaModel.from_pretrained(PRETRAINED_MODEL ,output_hidden_states = True)\n",
    "\n",
    "        # put this in eval mode so since we do not plan to do backprop and also any other special handling that it needs to do like dropout\n",
    "        model.eval()\n",
    "\n",
    "        # run sentences through the tokenizer\n",
    "        (\n",
    "            input_ids,\n",
    "            attention_masks,\n",
    "            attention_masks_without_special_tok,\n",
    "        ) = preprocessing_for_bert(texts, tokenizer, max_length=max_length)\n",
    "\n",
    "        # call the model on the sentences\n",
    "        outputs = model(input_ids, attention_masks)  # (tokenized_tensor, sent_tensor)\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "        print(\"Total hidden layers:\", len(hidden_states))\n",
    "        print(\n",
    "            \"First layer : hidden_states[0].shape \", hidden_states[0].shape\n",
    "        )  # [batch_size x seq_length x vector_dim]\n",
    "\n",
    "        # Lengths of each sentence\n",
    "        sent_lengths = attention_masks.sum(1).tolist()\n",
    "        # get tokenized sentences\n",
    "        tokenized_sents = [tokenizer.convert_ids_to_tokens(i) for i in input_ids]\n",
    "\n",
    "        MODE = mode\n",
    "\n",
    "        # if save dir not exit, make it\n",
    "\n",
    "        if not os.path.exists(f\"{save_dir}\"):\n",
    "            os.makedirs(f\"{save_dir}\")\n",
    "\n",
    "        # we save the file dynamically based on the models name, but want to remove any forward or backward slashes with _ to avoid the save function thinking its a directory\n",
    "        model_name = model_name.replace(\"/\", \"_\")\n",
    "\n",
    "        plots = eval_context_vecs(\n",
    "            hidden_states,\n",
    "            tokenized_sents,\n",
    "            model_name=model_name,\n",
    "            reducer=reducer,\n",
    "            mode=MODE,\n",
    "            max_length=max_length,\n",
    "            sent_lengths=sent_lengths,\n",
    "            words_with_diff_color=focus_word,\n",
    "            dims=dims,\n",
    "            save_dir=save_dir,\n",
    "        )\n",
    "        return plots\n",
    "\n",
    "    elif len(model_names) > 1:\n",
    "        # set dictionary to append plots to\n",
    "        plots = {}\n",
    "        for model_name in model_names:\n",
    "            print(f\"Working on: {model_name}\")\n",
    "            # Initialize tokenizer\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "            # RobertaTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "\n",
    "            # initialize model\n",
    "            # output_hidden_states = True will give us all hiddent states for all layers\n",
    "            model = AutoModel.from_pretrained(\n",
    "                model_name, output_hidden_states=True, cache_dir=cache_dir\n",
    "            )\n",
    "            # RobertaModel.from_pretrained(PRETRAINED_MODEL ,output_hidden_states = True)\n",
    "\n",
    "            # put this in eval mode so since we do not plan to do backprop and also any other special handling that it needs to do like dropout\n",
    "            model.eval()\n",
    "\n",
    "            # run sentences through the tokenizer\n",
    "            (\n",
    "                input_ids,\n",
    "                attention_masks,\n",
    "                attention_masks_without_special_tok,\n",
    "            ) = preprocessing_for_bert(texts, tokenizer, max_length=max_length)\n",
    "\n",
    "            # call the model on the sentences using the attention masks which will mean no attention paid to cls and seg tokens\n",
    "            outputs = model(\n",
    "                input_ids, attention_masks_without_special_tok\n",
    "            )  # (tokenized_tensor, sent_tensor)\n",
    "            hidden_states = outputs[2]\n",
    "\n",
    "            # print(\"Total hidden layers:\", len(hidden_states))\n",
    "            # print(\"First layer : hidden_states[0].shape \", hidden_states[0].shape)     # [batch_size x seq_length x vector_dim]\n",
    "\n",
    "            # Lengths of each sentence using original attention mask - needs to just avoid any pad tokens basically\n",
    "            sent_lengths = attention_masks.sum(1).tolist()\n",
    "            # get tokenized sentences\n",
    "            tokenized_sents = [tokenizer.convert_ids_to_tokens(i) for i in input_ids]\n",
    "\n",
    "            MODE = mode\n",
    "\n",
    "            # remove forward or backward slashes from model names to allow saving\n",
    "            model_name = model_name.replace(\"/\", \"_\")\n",
    "\n",
    "            plot = eval_context_vecs(\n",
    "                hidden_states,\n",
    "                tokenized_sents,\n",
    "                model_name=model_name,\n",
    "                reducer=reducer,\n",
    "                mode=MODE,\n",
    "                max_length=max_length,\n",
    "                sent_lengths=sent_lengths,\n",
    "                words_with_diff_color=focus_word,\n",
    "                dims=dims,\n",
    "                save_dir=save_dir,\n",
    "            )\n",
    "            plots[model_name] = plot\n",
    "\n",
    "        # if save dir not exit, make it\n",
    "\n",
    "        if not os.path.exists(f\"{save_dir}\"):\n",
    "            os.makedirs(f\"{save_dir}\")\n",
    "        # combine plots into list\n",
    "        fig_model_names = list(plots.keys())\n",
    "        # figures\n",
    "        figures = plots.values()\n",
    "\n",
    "        # make subplot\n",
    "        all_plots = make_subplots(\n",
    "            rows=1,\n",
    "            cols=2,\n",
    "            subplot_titles=fig_model_names,\n",
    "            specs=[[{\"type\": \"scene\"}, {\"type\": \"scene\"}]],\n",
    "        )\n",
    "\n",
    "        # need to pull out traces per figure\n",
    "        for i, figure in enumerate(figures):\n",
    "            for trace in range(len(figure[\"data\"])):\n",
    "                all_plots.append_trace(figure[\"data\"][trace], row=1, col=i + 1)\n",
    "\n",
    "        # save multi plot to file\n",
    "        all_plots.write_html(\n",
    "            f\"{save_dir}/multi_{'_'.join(fig_model_names)}_compare_{dims}D.html\"\n",
    "        )\n",
    "\n",
    "        return all_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f3629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up constants\n",
    "# Define some constants\n",
    "CACHE_DIR = (\n",
    "    \".cache\"  # set to whichever cache dir you want for downloaded transformer models\n",
    ")\n",
    "MODEL_NAME = [\"bert-base-uncased\"]  #'roberta-large'\n",
    "MAX_LENGTH = 15\n",
    "\n",
    "# I picked these sentences to see how if I really get different word vectors for \"heart\" in different contexts\n",
    "TEXTS = [\n",
    "    \"He did not have to heart to tell them\",\n",
    "    \"The patient had recently experience a heart attack\",\n",
    "    \"felt chest pain during the night\",\n",
    "]\n",
    "\n",
    "# this defines what I would like highlighted when I visualize the word vectors\n",
    "FOCUS_WORD = [\"heart\", \"chest\"]\n",
    "\n",
    "# the dimensionality reduction algorithm to use\n",
    "REDUCER = \"MDS\"\n",
    "\n",
    "# the word embedding extraction method\n",
    "MODE = \"concat\"\n",
    "\n",
    "SAVE_DIR = \"./plots/\"\n",
    "\n",
    "DIMS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7259d556",
   "metadata": {},
   "source": [
    "# bert base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fb7f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_plot = run_visualise_pipeline(\n",
    "    texts=TEXTS,\n",
    "    focus_word=FOCUS_WORD,\n",
    "    model_names=MODEL_NAME,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    max_length=MAX_LENGTH,\n",
    "    reducer=REDUCER,\n",
    "    dims=DIMS,\n",
    "    mode=MODE,\n",
    "    save_dir=SAVE_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ca9768",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e594f6",
   "metadata": {},
   "source": [
    "# roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11992742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try same text etc but different model\n",
    "MODEL_NAME = [\"roberta-base\"]  #'roberta-large'\n",
    "roberta_plot = run_visualise_pipeline(\n",
    "    texts=TEXTS,\n",
    "    focus_word=FOCUS_WORD,\n",
    "    model_names=MODEL_NAME,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    max_length=MAX_LENGTH,\n",
    "    reducer=REDUCER,\n",
    "    dims=DIMS,\n",
    "    mode=MODE,\n",
    "    save_dir=SAVE_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66655b27",
   "metadata": {},
   "source": [
    "# biomed roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca46d93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try same text etc but different model\n",
    "MODEL_NAME = [\"allenai/biomed_roberta_base\"]  #'roberta-large'\n",
    "run_visualise_pipeline(\n",
    "    texts=TEXTS,\n",
    "    focus_word=FOCUS_WORD,\n",
    "    model_names=MODEL_NAME,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    max_length=MAX_LENGTH,\n",
    "    reducer=REDUCER,\n",
    "    dims=DIMS,\n",
    "    mode=MODE,\n",
    "    save_dir=SAVE_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ba9057",
   "metadata": {},
   "source": [
    "# Try subplots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e741a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine plots into list\n",
    "# figures = [bert_plot, roberta_plot]\n",
    "\n",
    "# # make subplot\n",
    "# fig = make_subplots(rows=1, cols=2, subplot_titles=[\"Bert-base-uncased\", \"roberta-base\"])\n",
    "\n",
    "# # need to pull out traces per figure\n",
    "# for i, figure in enumerate(figures):\n",
    "#     for trace in range(len(figure[\"data\"])):\n",
    "#         fig.append_trace(figure[\"data\"][trace], row = 1, col = i+1)\n",
    "\n",
    "# fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db524c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try multiple models\n",
    "MODEL_NAMES = [\"allenai/biomed_roberta_base\", \"roberta-base\"]\n",
    "multi_plots = run_visualise_pipeline(\n",
    "    texts=TEXTS,\n",
    "    focus_word=FOCUS_WORD,\n",
    "    model_names=MODEL_NAMES,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    max_length=MAX_LENGTH,\n",
    "    reducer=REDUCER,\n",
    "    dims=DIMS,\n",
    "    mode=MODE,\n",
    "    save_dir=SAVE_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af16443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eef1a5",
   "metadata": {},
   "source": [
    "# Static word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aacc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_static_transformer_embedding(model, tokenizer, words):\n",
    "\n",
    "    # tokenize the word to get the id\n",
    "\n",
    "    word_embeddings = model.embeddings.word_embeddings.weight\n",
    "\n",
    "    # get the word or token ids for each\n",
    "\n",
    "    token_ids = tokenizer.encode(\n",
    "        words, add_special_tokens=False, is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    # now get the tokenized versions to act as the labels\n",
    "\n",
    "    tokenized_words = tokenizer.tokenize(words, is_split_into_words=True)\n",
    "\n",
    "    # now we can return these token ids and embeddings as a list or array\n",
    "\n",
    "    return tokenized_words, token_ids, word_embeddings[token_ids, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5d8e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_static_vecs(\n",
    "    model_names,\n",
    "    words,\n",
    "    cache_dir=\"<CACHE_DIR>\",\n",
    "    reducer=\"MDS\",\n",
    "    dims=2,\n",
    "    save_dir=\"./plots/\",\n",
    "):\n",
    "\n",
    "    # get model name\n",
    "    if len(model_names) == 1:\n",
    "\n",
    "        model_names = model_names[0]\n",
    "        # Initialize tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_names, cache_dir=cache_dir\n",
    "        )  # RobertaTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "\n",
    "        # initialize model\n",
    "        # output_hidden_states = True will give us all hiddent states for all layers\n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_names, output_hidden_states=True, cache_dir=cache_dir\n",
    "        )  # RobertaModel.from_pretrained(PRETRAINED_MODEL ,output_hidden_states = True)\n",
    "\n",
    "        # put this in eval mode so since we do not plan to do backprop and also any other special handling that it needs to do like dropout\n",
    "        model.eval()\n",
    "\n",
    "        # get word embeddings\n",
    "        tokenized_words, token_ids, word_embs = get_static_transformer_embedding(\n",
    "            model, tokenizer, words\n",
    "        )\n",
    "\n",
    "        # print(f\"{tokenized_words} ,  {token_ids}, {word_embs} of shape: {word_embs.shape}\")\n",
    "\n",
    "        # get cosine distances\n",
    "        # create a numpy matrix to pass to cosine distance\n",
    "        mat = word_embs.detach().numpy()\n",
    "\n",
    "        cosine_dists = cosine_distances(mat)\n",
    "\n",
    "        # we save the file dynamically based on the models name, but want to remove any forward or backward slashes with _ to avoid the save function thinking its a directory\n",
    "        model_name_save = model_names.replace(\"/\", \"_\")\n",
    "        # dists is precomputed using cosine similarity and passed\n",
    "\n",
    "        # calculate MDS with number of dims passed\n",
    "        if reducer == \"MDS\":\n",
    "            mds = manifold.MDS(\n",
    "                n_components=dims,\n",
    "                dissimilarity=\"precomputed\",\n",
    "                random_state=60,\n",
    "                max_iter=90000,\n",
    "            )\n",
    "            results = mds.fit(cosine_dists)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # get coodinates for each point\n",
    "        coords = results.embedding_\n",
    "\n",
    "        # set labels to tokenized words\n",
    "        labels = tokenized_words\n",
    "\n",
    "        # plot\n",
    "        if dims == 3:\n",
    "            fig = px.scatter_3d(\n",
    "                x=coords[:, 0],\n",
    "                y=coords[:, 1],\n",
    "                z=coords[:, 2],\n",
    "                text=labels,\n",
    "                color=labels,\n",
    "            )\n",
    "            # edit location of text for each point\n",
    "            fig.update_traces(textposition=\"top center\")\n",
    "            # remove legend\n",
    "            fig.update_layout(showlegend=False)\n",
    "\n",
    "        else:\n",
    "            fig = px.scatter(x=coords[:, 0], y=coords[:, 1], text=labels, color=labels)\n",
    "            # edit location of text for each point\n",
    "            fig.update_traces(textposition=\"top center\")\n",
    "            # remove legend\n",
    "            fig.update_layout(showlegend=False)\n",
    "\n",
    "        # set title\n",
    "        title = f\"Model: {model_names} and Method: static\"\n",
    "        if title != \"\":\n",
    "            fig.update_layout(title_text=title)\n",
    "        # save to html for later use?\n",
    "        save_dir = f\"{save_dir}/{model_name_save}_static_plotly_test_{dims}D.html\"\n",
    "        if save_dir != None:\n",
    "            fig.write_html(f\"{save_dir}\")\n",
    "\n",
    "        return fig\n",
    "\n",
    "    else:\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91dd61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = plot_static_vecs(\n",
    "    model_names=[\"bert-base-uncased\"],\n",
    "    words=[\n",
    "        \"one\",\n",
    "        \"two\",\n",
    "        \"three\",\n",
    "        \"queen\",\n",
    "        \"king\",\n",
    "        \"amputate\",\n",
    "        \"hospital\",\n",
    "        \"ambulance\",\n",
    "        \"cheese\",\n",
    "        \"accident\",\n",
    "        \"medical\",\n",
    "        \"prince\",\n",
    "        \"princess\",\n",
    "        \"royal\",\n",
    "    ],\n",
    "    dims=3,\n",
    "    cache_dir=\".cache\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4759f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "elm4psir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:53:40) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "036f9b356400688fa32ab139d64151f7af42c87240ca002d464048bf8c685a85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
