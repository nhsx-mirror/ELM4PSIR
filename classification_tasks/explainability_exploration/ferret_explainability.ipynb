{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6d23b51",
   "metadata": {},
   "source": [
    "### Ferret Explainability Tutorial\n",
    "\n",
    "This notebook is based around the tutorial provided by the package authors [here](https://github.com/g8a9/ferret). \n",
    "\n",
    "As with the other notebooks in this repo, we applied this to a patient safety set of fictional examples, but these steps can be easily applied to any classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bad767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to add a pipeline using ferret explainability\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from ferret import Benchmark, LIMEExplainer, SHAPExplainer\n",
    "from IPython.display import display\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# add the sys path for models\n",
    "sys.path.append(\"../\")\n",
    "from models.transformer_plms.hf_transformer_classifier import IncidentModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c55e08",
   "metadata": {},
   "source": [
    "__WARNING__ This framework will have been tested and designed to work with the AutoModelForSequenceClassification class - so make sure to load in that style of model to avoid issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df26710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model dir for trained models\n",
    "model_dir = \"./model/\"\n",
    "\n",
    "# set cache dir for transformer models\n",
    "cache_dir = \".cache_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add1ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    f\"{model_dir}\", cache_dir=cache_dir\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"{model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4601ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fc5c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9fe248",
   "metadata": {},
   "source": [
    "## Explain a single instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f5f73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = Benchmark(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fe002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_text = (\n",
    "# \"Patient was left waiting with a very high blood pressure for longer than advised. \"\n",
    "# \"Patient seemed very agitated by the experience\"\n",
    "# )\n",
    "example_text = \"The patient fell out of bed and broke their leg\"\n",
    "\n",
    "examples = [\n",
    "    \"The patient fell out of bed and broke their femur\",\n",
    "    \"The patient fell out of bed and was helped back up by a nurse\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c628931",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations = bench.explain(example_text, target=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf16f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f04709",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = bench.show_table(explanations)\n",
    "t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9d64582",
   "metadata": {},
   "source": [
    "## Evaluate explanation of a single instance\n",
    "\n",
    "Evaluate explanations with all the supported evaluators is straightforward. Remember to specify the `target` parameter to match the one used during the explanation!\n",
    "\n",
    "Area Over the Perturbation Curve (AOPC) Comprehensiveness (`aopc_compr`), AOPC Sufficiency (`aopc_suff`) and Correlation with Leave-One-Out scores (`taucorr_loo`) are three measures of faithfulness.\n",
    "\n",
    "- **AOPC Comprehensiveness**. Comprehensiveness measures the drop in the model probability if the relevant tokens of the explanations are removed. We measure comprehensiveness via the Area Over the Perturbation Curve by progressively considering the most $k$ important tokens, with $k$ from 1 to #tokens (as default) and then averaging the result. The higher the value, the more the explainer is able to select the relevant tokens for the prediction.\n",
    "\n",
    "- **AOPC Sufficiency**. Sufficiency captures if the tokens in the explanation are sufficient for the model to make the prediction. As for comprehensiveness, we use the AOPC score.\n",
    "\n",
    "- **Correlation with Leave-One-Out scores**. We first compute the leave-one-out scores by computing the prediction difference when one feature at the time is omitted. We then measure the Spearman correlation with the explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b341d241",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_evaluations = bench.evaluate_explanations(explanations, target=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6019e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.show_evaluation_table(explanation_evaluations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f504df3d",
   "metadata": {},
   "source": [
    "Given ground truth explanations - which are ultimately the tokens from the passage that are deemed as important to the decision in this case, we can calcualte some futher metrics. The explanations are provided as a vector similar to attention mask, with 0s for non-important tokens and 1s for important tokens. \n",
    "\n",
    "The bench class will (I think) expect a human rationale of the `length(tokenizer.encode(text)-2)`, essentially the number of tokens minus the bos and eos tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c93dd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9456fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.encode(example_text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4eb84888",
   "metadata": {},
   "source": [
    "### Plausability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72addf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_evaluations = bench.evaluate_explanations(\n",
    "    explanations, target=1, human_rationale=[0, 1, 0, 0], top_k_rationale=1\n",
    ")\n",
    "bench.show_evaluation_table(explanation_evaluations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "088587fc",
   "metadata": {},
   "source": [
    "Plausibility evaluates how well the explanation agree with human rationale. We evaluate plausibility via \n",
    "Area Under the Precision Recall curve (AUPRC) (`auprc_plau`),  token-level f1-score (`token_f1_plau`) and average Intersection-Over-Union (`IOU`) at the token level (`token_iou_plau`).\n",
    "\n",
    "- **Area Under the Precision Recall curve (AUPRC)** is computed by sweeping a threshold over token scores.\n",
    "- **Token-level f1-score** and the **`**average Intersection-Over-Union** consider discrete rationales.\n",
    "We derive a discrete rationale by taking the top-k values. K in the example is set to 1.\n",
    "- **Token-level f1-score** is the token-level F1 scores derived from the token-level precision and recall. \n",
    "- **Intersection-Over-Union (IOU)** is the size of the overlap of the tokens they cover divided by the size of their union.\n",
    "\n",
    "When the set of human rationales for the dataset is available, K is set as the average rationale length (as in ERASER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eea444",
   "metadata": {},
   "source": [
    "**Interface to individual explainers**\n",
    "\n",
    "You can also use individual explainers using an object oriented interface."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92cccad0",
   "metadata": {},
   "source": [
    "### LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7720a92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_exp = LIMEExplainer(model, tokenizer)\n",
    "lime_values = lime_exp(example_text, target=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f76a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a958662",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7620d7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_exp = SHAPExplainer(model, tokenizer)\n",
    "shap_values = shap_exp(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622bae5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "elm4psir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:53:40) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "036f9b356400688fa32ab139d64151f7af42c87240ca002d464048bf8c685a85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
