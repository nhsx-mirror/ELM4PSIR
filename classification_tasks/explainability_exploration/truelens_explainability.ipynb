{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f82b9cf6",
   "metadata": {},
   "source": [
    "### TruLens explainability\n",
    "\n",
    "This is a notebook based on the tutorial from the original [trulens-repo](https://github.com/truera/trulens) with a patient safety incident report set of examples - all entirely fictitious, and not neccessarily accurate. The main purpose of this notebook is to provide a quick guide to using `trulens`.\n",
    "\n",
    "This repo has a couple of different ways to train and setup transformer based classification models - one using the Transformers AutoModelForSequenceClassification classes, and the other with a customised classification heads etc.\n",
    "\n",
    "Annoyingly this requires some careful consideration when taking these models and loading them into other frameworks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5862e989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from trulens.utils.nlp import token_baseline\n",
    "\n",
    "# add the sys path for models\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from models.transformer_plms.hf_transformer_classifier import IncidentModel\n",
    "from trulens.nn.attribution import Cut, IntegratedGradients, OutputCut\n",
    "\n",
    "# Now wrap in TruLens classes\n",
    "from trulens.nn.models import get_model_wrapper\n",
    "from trulens.nn.quantities import ClassQoI\n",
    "from trulens.utils.typing import ModelInputs\n",
    "from trulens.visualizations import NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d739551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory for any custom models\n",
    "model_dir = \"./models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df50a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up some arguments dependent on the format of the model being loaded in\n",
    "if \"autoforsequence\" in model_dir:\n",
    "    model_type = \"autoforsequence\"\n",
    "else:\n",
    "    model_type = \"customclassifier\"\n",
    "\n",
    "if \"pretrained_format\" in model_dir:\n",
    "    pretrained_format = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31a93d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_type, pretrained_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c04b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap all of the necessary components.\n",
    "class IncidentSeverity:\n",
    "\n",
    "    device = \"cpu\"\n",
    "    # Can also use cuda if available:\n",
    "    # device = 'cuda:0'\n",
    "\n",
    "    if model_type == \"autoforsequence\":\n",
    "        if pretrained_format:\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(model_dir).to(\n",
    "                device\n",
    "            )\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "        else:\n",
    "            # load from IncidentModel class instead\n",
    "            model = IncidentModel.load_from_checkpoint(\n",
    "                f\"{model_dir}/best-checkpoint.ckpt\", model_type=model_type\n",
    "            )\n",
    "    else:\n",
    "        model = IncidentModel.load_from_checkpoint(\n",
    "            f\"{model_dir}/best-checkpoint.ckpt\", model_type=model_type\n",
    "        )\n",
    "\n",
    "        # tokenizer name\n",
    "        tokenizer_model_name = model.model.config.name_or_path\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name)\n",
    "\n",
    "    labels = [\"low\", \"high\"]\n",
    "\n",
    "    NEGATIVE = labels.index(\"low\")\n",
    "\n",
    "    POSITIVE = labels.index(\"high\")\n",
    "\n",
    "\n",
    "task = IncidentSeverity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def05cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"patient was left waiting with a very high blood pressure for longer than advised. Patient seemed very agitated by the experience\",\n",
    "    \"Patient was left waiting for 10 minutes\",\n",
    "    \"Nothing out of the ordinary\",\n",
    "]\n",
    "\n",
    "# Input sentences need to be tokenized first.\n",
    "\n",
    "inputs = task.tokenizer(sentences, padding=True, return_tensors=\"pt\").to(task.device)\n",
    "# pt refers to pytorch tensor\n",
    "\n",
    "# The tokenizer gives us vocabulary indexes for each input token (in this case,\n",
    "# words and some word parts like the \"'m\" part of \"I'm\" are tokens).\n",
    "\n",
    "print(inputs)\n",
    "\n",
    "# Decode helps inspecting the tokenization produced:\n",
    "\n",
    "print(task.tokenizer.batch_decode(torch.flatten(inputs[\"input_ids\"])))\n",
    "# Normally decode would give us a single string for each sentence but we would\n",
    "# not be able to see some of the non-word tokens there. Flattening first gives\n",
    "# us a string for each input_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ce4a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = task.model(**inputs)\n",
    "\n",
    "print(outputs)\n",
    "\n",
    "# From logits we can extract the most likely class for each sentence and its readable label.\n",
    "\n",
    "predictions = [task.labels[i] for i in outputs.logits.argmax(axis=1)]\n",
    "\n",
    "for sentence, logits, prediction in zip(sentences, outputs.logits, predictions):\n",
    "    print(logits.to(\"cpu\").detach().numpy(), prediction, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e92fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "task.wrapper = get_model_wrapper(task.model, device=task.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d679c45",
   "metadata": {},
   "source": [
    "# Attributions\n",
    "\n",
    "Applying integrated gradents to the sentiment model is similar as in the prior notebooks except special considerations need to be made for the cuts used as the targets of the attribution (i.e. what do we want to assign importance to). As you may have noted above, the model takes as input integer indexes associated with tokens. As we cannot take gradient with respect to these, we use an alternative: the embedding representation of those same inputs. To instantiate trulens with this regard, we need to find inspect the layer names inside our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a29a2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "task.wrapper.print_layer_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea3ea6",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7060790c",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Above, `roberta_embeddings_word_embeddings` or `model_embeddings_word_embeddings` is the layer that produces a continuous representation of each input token so we will use that layer as the one defining the **distribution of interest**. While most neural NLP models contain a token embedding, the layer name will differ.\n",
    "\n",
    "The second thing to note is the form of model outputs. Specifically, outputs are structures which contain a 'logits' attribute that stores the model scores.\n",
    "\n",
    "Putting these things together, we instantiate `IntegratedGradients` to attribute each embedding dimension to the maximum class (i.e. the predicted class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09a2265",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds_layer_name = \"roberta_embeddings_word_embeddings\"\n",
    "infl_max = IntegratedGradients(\n",
    "    model=task.wrapper,\n",
    "    doi_cut=Cut(f\"{embeds_layer_name}\"),\n",
    "    qoi_cut=OutputCut(accessor=lambda o: o[\"logits\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b1f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively we can look at a particular class:\n",
    "\n",
    "infl_positive = IntegratedGradients(\n",
    "    model=task.wrapper,\n",
    "    doi_cut=Cut(f\"{embeds_layer_name}\"),\n",
    "    qoi=ClassQoI(task.POSITIVE),\n",
    "    qoi_cut=OutputCut(accessor=lambda o: o[\"logits\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e7d3e1",
   "metadata": {},
   "source": [
    "Getting attributions uses the same call as model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162c68e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = infl_max.attributions(**inputs)\n",
    "\n",
    "for token_ids, token_attr in zip(inputs[\"input_ids\"], attrs):\n",
    "    for token_id, token_attr in zip(token_ids, token_attr):\n",
    "        # Not that each `word_attr` has a magnitude for each of the embedding\n",
    "        # dimensions, of which there are many. We aggregate them for easier\n",
    "        # interpretation and display.\n",
    "        attr = token_attr.sum()\n",
    "\n",
    "        word = task.tokenizer.decode(token_id)\n",
    "\n",
    "        print(f\"{word}({attr:0.3f})\", end=\" \")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92dff17",
   "metadata": {},
   "source": [
    "A listing as above is not very readable so Trulens comes with some utilities to present token influences a bit more concisely. First we need to set up a few parameters to make use of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35faa3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = NLP(\n",
    "    wrapper=task.wrapper,\n",
    "    labels=task.labels,\n",
    "    decode=lambda x: task.tokenizer.decode(x),\n",
    "    tokenize=lambda sentences: ModelInputs(\n",
    "        kwargs=task.tokenizer(sentences, padding=True, return_tensors=\"pt\")\n",
    "    ).map(lambda t: t.to(task.device)),\n",
    "    # huggingface models can take as input the keyword args as per produced by their tokenizers.\n",
    "    input_accessor=lambda x: x.kwargs[\"input_ids\"],\n",
    "    # for huggingface models, input/token ids are under input_ids key in the input dictionary\n",
    "    output_accessor=lambda x: x[\"logits\"],\n",
    "    # and logits under 'logits' key in the output dictionary\n",
    "    hidden_tokens=set([task.tokenizer.pad_token_id])\n",
    "    # do not display these tokens\n",
    ")\n",
    "\n",
    "print(\"QOI = MAX PREDICTION\")\n",
    "display(V.token_attribution(sentences, infl_max))\n",
    "\n",
    "print(\"QOI = POSITIVE\")\n",
    "display(V.token_attribution(sentences, infl_positive))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0469a8bc",
   "metadata": {},
   "source": [
    "# Baselines\n",
    "\n",
    "We see in the above results that special tokens such as the sentence end **&lt;/s&gt;** contributes are found to contribute a lot to the model outputs. While this may be useful in some contexts, we are more interested in the contributions of the actual words in these sentences. To focus on the words more, we need to adjust the **baseline** used in the integrated gradients computation. By default in the instantiation so far, the baseline for each token is a zero vector of the same shape as its embedding. By making the basaeline be identicaly to the explained instances on special tokens, we can rid their impact from our measurement. Trulens provides a utility for this purpose in terms of `token_baseline` which constructs for you the methods to compute the appropriate baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339fad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this needs to called differently based on whether it is using the AutoModel.from_pretrained class or not\n",
    "if pretrained_format:\n",
    "    inputs_baseline_ids, inputs_baseline_embeddings = token_baseline(\n",
    "        keep_tokens=set([task.tokenizer.cls_token_id, task.tokenizer.sep_token_id]),\n",
    "        # Which tokens to preserve.\n",
    "        replacement_token=task.tokenizer.pad_token_id,\n",
    "        # What to replace tokens with.\n",
    "        input_accessor=lambda x: x.kwargs[\"input_ids\"],\n",
    "        # this model.model.get_input... is for InstanceModel classes as the PLM component is inside the a sub-class named model...\n",
    "        ids_to_embeddings=task.model.get_input_embeddings()\n",
    "        # Callable to produce embeddings from token ids.\n",
    "    )\n",
    "else:\n",
    "\n",
    "    inputs_baseline_ids, inputs_baseline_embeddings = token_baseline(\n",
    "        keep_tokens=set([task.tokenizer.cls_token_id, task.tokenizer.sep_token_id]),\n",
    "        # Which tokens to preserve.\n",
    "        replacement_token=task.tokenizer.pad_token_id,\n",
    "        # What to replace tokens with.\n",
    "        input_accessor=lambda x: x.kwargs[\"input_ids\"],\n",
    "        # this model.model.get_input... is for InstanceModel classes as the PLM component is inside the a sub-class named model...\n",
    "        ids_to_embeddings=task.model.model.get_input_embeddings()\n",
    "        # Callable to produce embeddings from token ids.\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc76eedb",
   "metadata": {},
   "source": [
    "We can now inspect the baselines on some example sentences. The first method returned by `token_baseline` gives us token ids to inspect while the second gives us the embeddings of the baseline which we will pass to the attributions method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3c5548",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"originals=\", task.tokenizer.batch_decode(inputs[\"input_ids\"]))\n",
    "\n",
    "baseline_word_ids = inputs_baseline_ids(\n",
    "    model_inputs=ModelInputs(args=[], kwargs=inputs)\n",
    ")\n",
    "print(\"baselines=\", task.tokenizer.batch_decode(baseline_word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cadcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "infl_max_baseline = IntegratedGradients(\n",
    "    model=task.wrapper,\n",
    "    resolution=50,\n",
    "    baseline=inputs_baseline_embeddings,\n",
    "    doi_cut=Cut(f\"{embeds_layer_name}\"),\n",
    "    qoi_cut=OutputCut(accessor=lambda o: o[\"logits\"]),\n",
    ")\n",
    "\n",
    "\n",
    "infl_positive_baseline = IntegratedGradients(\n",
    "    model=task.wrapper,\n",
    "    resolution=50,\n",
    "    baseline=inputs_baseline_embeddings,\n",
    "    doi_cut=Cut(f\"{embeds_layer_name}\"),\n",
    "    qoi=ClassQoI(task.POSITIVE),\n",
    "    qoi_cut=OutputCut(accessor=lambda o: o[\"logits\"]),\n",
    ")\n",
    "\n",
    "\n",
    "print(\"QOI = MAX PREDICTION\")\n",
    "display(V.token_attribution(sentences, infl_max_baseline))\n",
    "\n",
    "print(\"QOI = POSITIVE WITH BASELINE\")\n",
    "display(V.token_attribution(sentences, infl_positive_baseline))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "elm4psir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:53:40) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "036f9b356400688fa32ab139d64151f7af42c87240ca002d464048bf8c685a85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
