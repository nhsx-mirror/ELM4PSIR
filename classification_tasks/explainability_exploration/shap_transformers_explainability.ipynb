{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f5d58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet shap==0.39"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901bbdf6",
   "metadata": {},
   "source": [
    "### SHAP with transformers\n",
    "\n",
    "This is a notebook providing a quick tutorial for loading and investigating the explainability of PLMs given a classification task such as sentiment using SHAP. Any classification task can be used really and *ideally*, the model should have been trained on that particular classification task. For more details on SHAP, see the package documentation [here](httsp://shap.readthedocs.io).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a10b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# import bios\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import shap\n",
    "import torch\n",
    "import transformers\n",
    "from numpy.lib.histograms import _histogram_dispatcher\n",
    "from torchnlp.encoders import Encoder\n",
    "from torchnlp.encoders.text import stack_and_pad_tensors\n",
    "from torchnlp.encoders.text.text_encoder import TextEncoder\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TextClassificationPipeline,\n",
    ")\n",
    "\n",
    "# from tokenizer import Tokenizer\n",
    "\n",
    "# add the sys path for models\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from models.transformer_plms.hf_transformer_classifier import IncidentModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef51159b",
   "metadata": {},
   "source": [
    "#### Using Trained Classifier from transformer AutoSequenceForClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f58ce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set ckpt path to model trained on the classification task of interest\n",
    "model_dir = \"./model/\"  # your directory to be put here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba0153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec189ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can print model to check its class\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8cede0",
   "metadata": {},
   "source": [
    "##### If using the automodelforseqeunce classification can use transformers pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec42420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the model label idx\n",
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc23e4cc",
   "metadata": {},
   "source": [
    "The transformer pipeline will only work with models with the untouched AutoModel class..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f35fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # using the transformers pipeline - set device=0 if cuda is wanted\n",
    "pipe = transformers.pipeline(\n",
    "    \"text-classification\", model=model, tokenizer=tokenizer, return_all_scores=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfbc503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function\n",
    "\n",
    "\n",
    "def score_and_visualize(text):\n",
    "\n",
    "    \"\"\"\n",
    "    Function:\n",
    "        Wrapper around the pipe class to return predictions and push through shap\n",
    "        explainer\n",
    "    \"\"\"\n",
    "\n",
    "    explainer = shap.Explainer(pipe)\n",
    "    shap_values = explainer(text)\n",
    "\n",
    "    shap.plots.text(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ee28fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the model's predictions\n",
    "example_texts = [\n",
    "    \"patient fell down and broke their leg\",\n",
    "    \"severe breathing problems\",\n",
    "    \"no idea\",\n",
    "]\n",
    "\n",
    "explainer = shap.Explainer(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f61319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on some examples\n",
    "shap_values = explainer(example_texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1cf8af72",
   "metadata": {},
   "source": [
    "Visualize the impact on all the output classes\n",
    "\n",
    "In the plots below, when you hover your mouse over an output class you get the explanation for that output class. When you click an output class name then that class remains the focus of the explanation visualization until you click another class.\n",
    "\n",
    "The base value is what the model outputs when the entire input text is masked, while\n",
    "is the output of the model for the full original input. The SHAP values explain in an addive way how the impact of unmasking each word changes the model output from the base value (where the entire input is masked) to the final prediction value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a600f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.text(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4808972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also use the wrapper\n",
    "score_and_visualize(example_texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e48cae4",
   "metadata": {},
   "source": [
    "### Below is manual to use without transformers pipeline - \n",
    "__NOTE__ it is recommended to try using the pipeline method above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054eac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prediction_gpu(x):\n",
    "    # shap expects this form of list comprehension - it breaks when using\n",
    "    # tokenizer as normal...\n",
    "    tv = torch.tensor(\n",
    "        [\n",
    "            tokenizer.encode(v, padding=\"max_length\", max_length=10, truncation=True)\n",
    "            for v in x\n",
    "        ]\n",
    "    ).cuda()\n",
    "    attention_mask = (tv != 0).type(torch.int64).cuda()\n",
    "    outputs = model(tv, return_dict=True)\n",
    "    logits = outputs.logits\n",
    "    scores = torch.nn.Softmax(dim=-1)(logits)\n",
    "    val = torch.logit(scores).detach().cpu().numpy()\n",
    "\n",
    "    return val\n",
    "\n",
    "\n",
    "def model_prediction_cpu(x):\n",
    "    tv = torch.tensor(\n",
    "        [\n",
    "            tokenizer.encode(v, padding=\"max_length\", max_length=10, truncation=True)\n",
    "            for v in x\n",
    "        ]\n",
    "    )\n",
    "    attention_mask = (tv != 0).type(torch.int64).cpu()\n",
    "    outputs = model(tv, return_dict=True)\n",
    "    logits = outputs.logits\n",
    "    scores = torch.nn.Softmax(dim=-1)(logits)\n",
    "    val = torch.logit(scores).detach().numpy()\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719ff9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "model_prediction_cpu([\"one two three\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcda3916",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "model_prediction_gpu([\"one two three\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a8d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\"one two three\", \"eight nice ten\"]\n",
    "encoded_inputs = torch.tensor(\n",
    "    [\n",
    "        tokenizer.encode(words, padding=\"max_length\", truncation=True, max_length=512)\n",
    "        for words in x\n",
    "    ]\n",
    ").cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa155ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b2d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_texts = [\n",
    "    \"patient was left waiting with a very high blood pressure for longer than advise\",\n",
    "    \"Patient was left waiting for 10 minutes\",\n",
    "    \"Nothing out of the ordinary\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e6b11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc0ad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpu explainer\n",
    "model.cpu()\n",
    "cpu_explainer = shap.Explainer(\n",
    "    model_prediction_cpu, tokenizer, output_names=[\"low\", \"high\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df14e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = cpu_explainer(example_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49cf7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.text(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5d2ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x[0] for x in sorted(model.model.config.label2id.items(), key=lambda x: x[1])]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "elm4psir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:53:40) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "036f9b356400688fa32ab139d64151f7af42c87240ca002d464048bf8c685a85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
